{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b84546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f1b28",
   "metadata": {},
   "source": [
    "í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì§ˆë¬¸ input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a2090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"ìë°”ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fc2a0",
   "metadata": {},
   "source": [
    "ê²°ê³¼ ë°›ì•„ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9435812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000024077E4C890> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000240793E91F0> root_client=<openai.OpenAI object at 0x0000024076804620> root_async_client=<openai.AsyncOpenAI object at 0x00000240795C7FB0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82e3b3",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì„œ ë‹µì„ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0303c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ë‹µ: ìë°”ëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. 1995ë…„ì— ì¬ ë§ˆì´í¬ë¡œì‹œìŠ¤í…œì¦ˆ(í˜„ì¬ ì˜¤ë¼í´ ì†Œìœ )ì—ì„œ ì œì„ìŠ¤ ê³ ìŠ¬ë§ì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ìë°”ëŠ” í”Œë«í¼ ë…ë¦½ì ì´ë©°, ë‹¤ì–‘í•œ ìš´ì˜ ì²´ì œì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°**: ìë°”ëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ì´ëŠ” í”„ë¡œê·¸ë¨ì„ ê°ì²´ë¡œ êµ¬ì„±í•˜ê³ , ê°ì²´ ê°„ì˜ ìƒí˜¸ ì‘ìš©ì„ í†µí•´ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
      "2. **í”Œë«í¼ ë…ë¦½ì„±**: ìë°”ëŠ” ìë°” ê°€ìƒ ë¨¸ì‹ (JVM)ì„ í†µí•´ í”Œë«í¼ ë…ë¦½ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. JVMì€ ìë°” ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê°€ìƒ ë¨¸ì‹ ìœ¼ë¡œ, ë‹¤ì–‘í•œ ìš´ì˜ ì²´ì œì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "3. **ê°„ê²°í•œ ë¬¸ë²•**: ìë°”ëŠ” ê°„ê²°í•œ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìë°”ëŠ” C++ê³¼ ìœ ì‚¬í•œ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, ë”ìš± ê°„ë‹¨í•˜ê³  ëª…í™•í•©ë‹ˆë‹¤.\n",
      "4. **ê°•ë ¥í•œ ë³´ì•ˆ**: ìë°”ëŠ” ê°•ë ¥í•œ ë³´ì•ˆ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ìë°”ëŠ” ë©”ëª¨ë¦¬ ê´€ë¦¬, ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬, ë³´ì•ˆ ì •ì±… ë“±ì„ í†µí•´ ë³´ì•ˆ ìœ„í˜‘ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
      "5. **ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°**: ìë°”ëŠ” ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìë°”ëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, ë§ì€ ê°œë°œìë“¤ì´ ìë°”ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì›¹ ê°œë°œ, ëª¨ë°”ì¼ ì•± ê°œë°œ, ë°ìŠ¤í¬í†± ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ, ê²Œì„ ê°œë°œ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ìë°” ì–¸ì–´**: ìë°” ì–¸ì–´ëŠ” ìë°”ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ìë°” ì–¸ì–´ëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì„ ì§€ì›í•˜ë©°, ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "2. **ìë°” ê°€ìƒ ë¨¸ì‹ (JVM)**: JVMì€ ìë°” ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê°€ìƒ ë¨¸ì‹ ì…ë‹ˆë‹¤. JVMì€ í”Œë«í¼ ë…ë¦½ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "3. **ìë°” ë¼ì´ë¸ŒëŸ¬ë¦¬**: ìë°” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ìë°”ì˜ ê¸°ëŠ¥ì„ í™•ì¥í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ìë°” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ì˜ ë²„ì „ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ìë°” 1.0**: ìë°” 1.0ì€ ìë°”ì˜ ì²« ë²ˆì§¸ ë²„ì „ì…ë‹ˆë‹¤. 1995ë…„ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "2. **ìë°” 8**: ìë°” 8ì€ ìë°”ì˜ ìµœì‹  ë²„ì „ì…ë‹ˆë‹¤. 2014ë…„ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ëŠ” ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ìë°”ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ë©°, ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(\"ì‘ë‹µ:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173662bf",
   "metadata": {},
   "source": [
    "## LCEL\n",
    "Prompt + LLM Chainìœ¼ë¡œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206e159",
   "metadata": {},
   "source": [
    "1. PromptTemplate ì‚¬ìš©í•´ì„œ model í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c87d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an expert in AI Expert. Answer the question. <Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. \"\n",
    "                                \"<Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926a7b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain ì—°ê²°\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain ì—°ê²° (output_parser ì¶”ê°€)\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad6eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì‚¬ëŒì˜ ë‡Œê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. \n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**: ìš°ì„ , ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. \n",
      "\n",
      "2. **ë°ì´í„° ì „ì²˜ë¦¬**: ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ê¹¨ë—í•˜ê³  ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ëª¨ë¸ ì„¤ì •**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ì‹ ê²½ë§ì˜ ì¸µ ìˆ˜, ê° ì¸µì˜ ë‰´ëŸ° ìˆ˜, í™œì„±í™” í•¨ìˆ˜ ë“±ì˜ ê²°ì •ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
      "\n",
      "4. **í•™ìŠµ**: ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì…ë ¥í•˜ê³ , ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì‹¤ì œ ê²°ê³¼ ì‚¬ì´ì˜ ì˜¤ë¥˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë“¤(ê°€ì¤‘ì¹˜ì™€ í¸í–¥)ì„ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "5. **í‰ê°€**: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "6. **íŠœë‹**: ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šë‹¤ë©´, ëª¨ë¸ì˜ êµ¬ì¡°ë‚˜ í•™ìŠµ ë°©ë²•, ë°ì´í„°ì˜ ì „ì²˜ë¦¬ ê³¼ì • ë“±ì„ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ììœ¨ ì£¼í–‰ ìë™ì°¨ì˜ ê²½ìš°, ìˆ˜ë§ì€ ë„ë¡œ ìƒí™© ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ìë™ì°¨ê°€ ìŠ¤ìŠ¤ë¡œ ìƒí™©ì„ íŒë‹¨í•˜ê³  ìš´ì „í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
      "\n",
      "ì´ëŸ¬í•œ í•™ìŠµ ê³¼ì •ì„ í†µí•´ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥ì„ í‚¤ìš°ê²Œ ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Chain í˜¸ì¶œ\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ë°œìƒ : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8db8d",
   "metadata": {},
   "source": [
    "### Runnableì˜ stream()í•¨ìˆ˜ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "840000f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì‚¬ëŒì˜ ë‡Œê°€ í•™ìŠµí•˜ëŠ” ì›ë¦¬ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë‡ŒëŠ” ê²½í—˜ì„ í†µí•´ ë°°ìš°ê³ , ìƒˆë¡œìš´ ì •ë³´ë¥¼ ê¸°ì¡´ ì§€ì‹ê³¼ ì—°ê²°í•˜ì—¬ ê¸°ì–µí•©ë‹ˆë‹¤. ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë„ ë°ì´í„°ë¥¼ í†µí•´ ë°°ìš°ê³ , ê·¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ ì°¾ê³ , ë¯¸ë˜ì˜ ìƒˆë¡œìš´ ìƒí™©ì— ëŒ€ì²˜í•˜ëŠ” ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì§„ì„ ë³´ê³  ê³ ì–‘ì´ì¸ì§€ ê°œì¸ì§€ êµ¬ë¶„í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ë§Œë“ ë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. ì´ ëª¨ë¸ì—ê²Œ ìˆ˜ë§ì€ ê³ ì–‘ì´ì™€ ê°œì˜ ì‚¬ì§„ì„ ë³´ì—¬ì£¼ê³ , ì´ê²ƒì´ ê³ ì–‘ì´ì¸ì§€ ê°œì¸ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ëª¨ë¸ì´ ê³ ì–‘ì´ì™€ ê°œì˜ êµ¬ë¶„ì„ ì˜ ëª»í•˜ì§€ë§Œ, ì‚¬ì§„ì„ ë³¼ ë•Œë§ˆë‹¤ ì¡°ê¸ˆì”© í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ê´€ë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì´ ê²½ìš°, ê³ ì–‘ì´ì™€ ê°œì˜ ì‚¬ì§„ë“¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "2. **ë°ì´í„° ë¶„ì„**: ëª¨ë¸ì€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì€ ì‚¬ì§„ ì†ì—ì„œ ê³ ì–‘ì´ì™€ ê°œì˜ íŠ¹ì§•ì„ ì°¾ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ëŠ” ëˆˆì´ í¬ê³ , ê·€ê°€ ë¾°ì¡±í•˜ë©°, ê°œì˜ ê²½ìš° ê·€ê°€ ì³ì ¸ ìˆëŠ” ê²½ìš°ê°€ ë§ë‹¤ëŠ” ë“±ì˜ íŠ¹ì§•ì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ëª¨ë¸ ì—…ë°ì´íŠ¸**: ëª¨ë¸ì€ ë¶„ì„í•œ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ê³ ì–‘ì´ì™€ ê°œë¥¼ êµ¬ë¶„í•˜ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ, ë” ë§ì€ ë°ì´í„°ë¥¼ í•™ìŠµí• ìˆ˜ë¡ ì ì  ë” ì •í™•í•´ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì˜ˆì¸¡**: í•™ìŠµì´ ì™„ë£Œëœ í›„, ëª¨ë¸ì€ ìƒˆë¡œìš´ ì‚¬ì§„ì„ ë³´ê³  ê·¸ê²ƒì´ ê³ ì–‘ì´ì¸ì§€ ê°œì¸ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì´ëŸ¬í•œ í•™ìŠµ ê³¼ì •ì€ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë˜ë©°, ëª¨ë¸ì˜ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤. ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ëŸ¬í•œ ë°˜ë³µì ì¸ í•™ìŠµê³¼ ê°œì„  ê³¼ì •ì„ í†µí•´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•œ ìš”ì²­\n",
    "answer = chain2.stream({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬\"})\n",
    "# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "\n",
    "for token in answer:\n",
    "    # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë°›ì€ ë°ì´í„°ì˜ ë‚´ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ì„œ ì¶œë ¥í•˜ê³ , ë²„í¼ë¥¼ ì¦‰ì‹œ ë¹„ì›ë‹ˆë‹¤.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230c581",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "- prompt 1ì˜ ì¶œë ¥ì„ prompt 2ì˜ ì…ë ¥ì— ì‚¬ìš©í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¥ë¥´ì— ë”°ë¼ ì˜í™” ì¶”ì²œ\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} ì¥ë¥´ì—ì„œ ì¶”ì²œí•  ë§Œí•œ ì˜í™”ë¥¼ í•œ í¸ ì•Œë ¤ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# Step 2: ì¶”ì²œëœ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ ìš”ì•½\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} ì¶”ì „í•œ ì˜í™”ì˜ ì œëª©ì„ ë¨¼ì € ì•Œë ¤ì£¼ì‹œê³ , ì¤„ì„ ë°”ê¾¸ì–´ì„œ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ì²´ì¸ 1: ì˜í™” ì¶”ì²œ (ì…ë ¥: ì¥ë¥´ â†’ ì¶œë ¥: ì˜í™” ì œëª©)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64217427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  **ì¡´ ìœ… (John Wick)**\n",
      "2.  **ë¯¸ì…˜ ì„íŒŒì„œë¸”: ë°ë“œ ë ˆì½˜ë‹ íŒŒíŠ¸ ì›**\n",
      "\n",
      "*   **ì¡´ ìœ… (John Wick)**: ì€í‡´í•œ ì „ì§ ì•”ì‚´ì ì¡´ ìœ…ì€ ì€í‡´ í›„ í‰í™”ë¡œìš´ ì‚¶ì„ ì‚´ê³ ì í•˜ì§€ë§Œ, ê·¸ì˜ ì°¨ë¥¼ í›”ì¹˜ê³  ê°œë¥¼ ì£½ì¸ ê³ ì–´ê°±ì´ë¼ëŠ” ë²”ì£„ ì§‘ë‹¨ì´ ê·¸ì˜ ì‚¶ì— ìœ„í˜‘ì„ ê°€í•©ë‹ˆë‹¤. ì¡´ ìœ…ì€ ë³µìˆ˜ë¥¼ ìœ„í•´ ëª¨ë“  ê²ƒì„ í¬ìƒí•˜ë©° ì‹¸ì›ë‹ˆë‹¤. ê·¸ì˜ ì „ì„¤ì ì¸ ì‹¤ë ¥ê³¼ ì „íˆ¬ ê¸°ìˆ ì„ ë°”íƒ•ìœ¼ë¡œ ì¡´ ìœ…ì€ ë²”ì£„ ì¡°ì§ì— ë§ì„œ ì‹¸ìš°ë©°, ê°•ë ¥í•œ ì ë“¤ê³¼ì˜ ì¹˜ì—´í•œ ì „íˆ¬ë¥¼ ë²Œì…ë‹ˆë‹¤.\n",
      "*   **ë¯¸ì…˜ ì„íŒŒì„œë¸”: ë°ë“œ ë ˆì½˜ë‹ íŒŒíŠ¸ ì›**: ì „ ì„¸ê³„ì˜ í‰í™”ë¥¼ ìœ„í˜‘í•˜ëŠ” ê°•ë ¥í•œ ì¸ê³µì§€ëŠ¥ 'ì—”í‹°í‹°'ê°€ ë“±ì¥í•©ë‹ˆë‹¤. ë¹„ë°€ ì •ë³´ê¸°ê´€ IMFì˜ ì—ë‹¨ í—ŒíŠ¸(í†° í¬ë£¨ì¦ˆ)ëŠ” ë™ë£Œë“¤ê³¼ í•¨ê»˜ ì—”í‹°í‹°ë¥¼ íŒŒê´´í•˜ê¸° ìœ„í•œ ìœ„í—˜í•œ ì„ë¬´ì— ë‚˜ì„­ë‹ˆë‹¤. ì—ë‹¨ í—ŒíŠ¸ì™€ ê·¸ì˜ íŒ€ì€ ì—”í‹°í‹°ë¥¼ ë¬´ì°Œë¥´ê³  ì„¸ê³„ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ëª©ìˆ¨ì„ ê±´ ì „íˆ¬ë¥¼ ë²Œì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì²´ì¸ 2: ì¤„ê±°ë¦¬ ìš”ì•½ (ì…ë ¥: ì˜í™” ì œëª© â†’ ì¶œë ¥: ì¤„ê±°ë¦¬)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1ì˜ ì¶œë ¥ì„ movie ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰: \"SF\" ì¥ë¥´ì˜ ì˜í™” ì¶”ì²œ ë° ì¤„ê±°ë¦¬ ìš”ì•½\n",
    "response = chain2.invoke({\"genre\": \"ì•¡ì…˜\"})\n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463612fb",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d5313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deep learning is a subset of machine learning, which is a type of artificial intelligence (AI). It involves the use of artificial neural networks to analyze data and make decisions. The term \"deep\" refers to the fact that these neural networks are composed of multiple layers, which allow them to learn complex patterns and relationships in data.\n",
      "\n",
      "**Key Characteristics of Deep Learning:**\n",
      "\n",
      "* **Artificial Neural Networks (ANNs):** Deep learning models are based on ANNs, which are inspired by the structure and function of the human brain.\n",
      "* **Multiple Layers:** Deep learning models have multiple layers of interconnected nodes or \"neurons,\" which process and transform inputs to produce outputs.\n",
      "* **Hierarchical Representations:** Each layer in a deep learning model learns to represent the input data at a higher level of abstraction, allowing the model to learn complex patterns and relationships.\n",
      "\n",
      "**How Deep Learning Works:**\n",
      "\n",
      "1. **Data Input:** The model receives input data, such as images, text, or audio.\n",
      "2. **Forward Propagation:** The input data flows through the layers of the model, with each layer applying a set of weights and biases to the input data.\n",
      "3. **Activation Functions:** Each layer applies an activation function to the output, which determines the output of the layer.\n",
      "4. **Backward Propagation:** The model computes the error between its predictions and the actual output, and then propagates the error backwards through the layers to update the weights and biases.\n",
      "5. **Optimization:** The model is optimized using an optimization algorithm, such as stochastic gradient descent (SGD), to minimize the error and improve its performance.\n",
      "\n",
      "**Applications of Deep Learning:**\n",
      "\n",
      "* **Computer Vision:** Image classification, object detection, segmentation, and generation.\n",
      "* **Natural Language Processing (NLP):** Text classification, language translation, and text generation.\n",
      "* **Speech Recognition:** Speech-to-text and voice recognition.\n",
      "* **Robotics:** Control and navigation of robots.\n",
      "\n",
      "**Types of Deep Learning Models:**\n",
      "\n",
      "* **Convolutional Neural Networks (CNNs):** Used for image and video processing.\n",
      "* **Recurrent Neural Networks (RNNs):** Used for sequential data, such as text, speech, and time series data.\n",
      "* **Generative Adversarial Networks (GANs):** Used for generative modeling, such as generating new images or text.\n",
      "\n",
      "**Benefits of Deep Learning:**\n",
      "\n",
      "* **Improved Accuracy:** Deep learning models can achieve state-of-the-art performance on a wide range of tasks.\n",
      "* **Flexibility:** Deep learning models can be applied to a variety of domains and tasks.\n",
      "* **Scalability:** Deep learning models can be trained on large datasets and can scale to complex tasks.\n",
      "\n",
      "**Challenges of Deep Learning:**\n",
      "\n",
      "* **Training Time:** Deep learning models can take a long time to train, especially on large datasets.\n",
      "* **Overfitting:** Deep learning models can suffer from overfitting, which occurs when the model is too complex and performs poorly on new, unseen data.\n",
      "* **Interpretability:** Deep learning models can be difficult to interpret, making it challenging to understand why a particular decision was made.\n",
      "\n",
      "Overall, deep learning is a powerful tool for building AI systems that can learn from data and make accurate predictions or decisions. Its applications are diverse and continue to grow, transforming industries and revolutionizing the way we live and work.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an {topic} expert in AI. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ebc39",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "ì˜ˆì‹œë¥¼ ì¤˜ì„œ ê²°ê³¼ê°’ì„ ìœ ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc415c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### íƒœì–‘ê³„ì˜ í–‰ì„±\n",
      "1. **ìˆ˜ì„±**: ê°€ì¥ ì‘ì€ í–‰ì„±, íƒœì–‘ê³¼ ê°€ê¹ìŠµë‹ˆë‹¤.\n",
      "2. **ê¸ˆì„±**: ë§¤ìš° ëœ¨ê²ê³  ë°ì€ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "3. **ì§€êµ¬**: ìƒëª…ì²´ê°€ ì‚¬ëŠ” ìœ ì¼í•œ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "4. **í™”ì„±**: ë¶‰ì€ìƒ‰ì´ë©°, ë¡œë´‡ íƒì‚¬ê°€ í™œë°œí•©ë‹ˆë‹¤.\n",
      "5. **ëª©ì„±**: íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "6. **í† ì„±**: ì•„ë¦„ë‹¤ìš´ ê³ ë¦¬ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "7. **ì²œì™•ì„±**: ìì „ì¶•ì´ ê¸°ìš¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
      "8. **í•´ì™•ì„±**: ê°€ì¥ ë¨¼ í–‰ì„±ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\"})\n",
    "#result = chain.invoke({\"input\": \"ì–‘ì ì–½í˜ì´ ë¬´ì—‡ì¸ê°€ìš”?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea33ea",
   "metadata": {},
   "source": [
    "### PartialPromptTemplate\n",
    "* í”„ë¡¬í”„íŠ¸ ì…ë ¥ ê°’ì— í•¨ìˆ˜ í˜¸ì¶œ ì´ë‚˜ ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•œ ë™ì ì¸ ê°’ì„ ëŒ€ì…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60093c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ê³„ì ˆ: ì—¬ë¦„\n",
      "ğŸ”¹ í”„ë¡¬í”„íŠ¸: input_variables=['season'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['season'], input_types={}, partial_variables={}, template='{season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”ê° í˜„ìƒì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”'), additional_kwargs={})]\n",
      "ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: ì—¬ë¦„ì— ë°œìƒí•˜ëŠ” ìì—°í˜„ìƒ: \n",
      " 1.  **ì¥ë§ˆ** ì¥ë§ˆëŠ” ì—¬ë¦„ì² ì— ë°œìƒí•˜ëŠ” í˜„ìƒìœ¼ë¡œ, í•œêµ­, ì¤‘êµ­, ì¼ë³¸ ë“± ë™ì•„ì‹œì•„ ì§€ì—­ì—ì„œ í”íˆ ë°œìƒí•©ë‹ˆë‹¤. ì¥ë§ˆëŠ” ë‚¨ìª½ì—ì„œ ë¶ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” ì €ê¸°ì••ê³¼ ê³ ê¸°ì••ì˜ ì˜í–¥ìœ¼ë¡œ ë°œìƒí•˜ë©°, ì´ë¡œ ì¸í•´ ë§ì€ ë¹„ê°€ ë‚´ë¦¬ê³  ìŠµë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤. ì¥ë§ˆëŠ” ì¼ë°˜ì ìœ¼ë¡œ 6ì›” ì¤‘ìˆœë¶€í„° 7ì›” ì¤‘ìˆœê¹Œì§€ ì§€ì†ë˜ë©°, ì´ ê¸°ê°„ ë™ì•ˆ ë§ì€ ë¹„ê°€ ë‚´ë¦¬ê³  í™ìˆ˜ë‚˜ ì‚°ì‚¬íƒœ ë“±ì˜ í”¼í•´ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "2.  **íƒœí’** íƒœí’ì€ ì—¬ë¦„ì² ì— ë°œìƒí•˜ëŠ” ê°•ë ¥í•œ ì €ê¸°ì••ìœ¼ë¡œ, ì—´ëŒ€ ì§€ë°©ì˜ ë°”ë‹¤ì—ì„œ ë°œìƒí•˜ì—¬ í•œêµ­, ì¼ë³¸, ì¤‘êµ­ ë“± ë™ì•„ì‹œì•„ ì§€ì—­ìœ¼ë¡œ ì´ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íƒœí’ì€ ê°•í•œ ë°”ëŒê³¼ ë§ì€ ë¹„ë¥¼ ë™ë°˜í•˜ë©°, ì´ë¡œ ì¸í•´ í° í”¼í•´ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íƒœí’ì€ ì¼ë°˜ì ìœ¼ë¡œ 7ì›” ì¤‘ìˆœë¶€í„° 9ì›” ì¤‘ìˆœê¹Œì§€ ë°œìƒí•˜ë©°, ì´ ê¸°ê°„ ë™ì•ˆ ë§ì€ íƒœí’ì´ ë™ì•„ì‹œì•„ ì§€ì—­ì„ ê°•íƒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "3.  **ê°€ë­„** ê°€ë­„ì€ ì—¬ë¦„ì² ì— ë°œìƒí•˜ëŠ” í˜„ìƒìœ¼ë¡œ, ì¼ë¶€ ì§€ì—­ì—ì„œ ë¹„ê°€ ì˜¤ì§€ ì•Šì•„ í† ì–‘ì´ ê±´ì¡°í•´ì§€ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ê°€ë­„ì€ ì¼ë°˜ì ìœ¼ë¡œ 7ì›” ì¤‘ìˆœë¶€í„° 9ì›” ì¤‘ìˆœê¹Œì§€ ì§€ì†ë˜ë©°, ì´ ê¸°ê°„ ë™ì•ˆ ë§ì€ ì§€ì—­ì—ì„œ ë¬¼ ë¶€ì¡±ê³¼ ë†ì‘ë¬¼ í”¼í•´ ë“±ì˜ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "    \n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì…ë‹ˆë‹¤.\",\n",
    "#     input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "#     partial_variables={\"season\": get_current_season()}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    "# )\n",
    "season = get_current_season()\n",
    "print(f\"í˜„ì¬ ê³„ì ˆ: {season}\")\n",
    "prompt = ChatPromptTemplate.from_template(\"{season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”\"\n",
    "                                          \"ê° í˜„ìƒì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "chain = (\n",
    "    {\"season\": lambda x: season} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({})\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸ”¹ í”„ë¡¬í”„íŠ¸: {prompt}\")\n",
    "print(f\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: {season}ì— ë°œìƒí•˜ëŠ” ìì—°í˜„ìƒ: \\n {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={'info': '1ë‹¬ëŸ¬ = 1374.55ì›'} template='í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.'\n"
     ]
    }
   ],
   "source": [
    "# APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ë™ì ì¸ ê°’ì„  partial variableë¡œ ì„¤ì •\n",
    "\n",
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# {info} ë³€ìˆ˜ì— APIì—ì„œ ë°›ì€ í™˜ìœ¨ ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ë°˜ì˜\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8ecadf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'season'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m model = ChatOpenAI(\n\u001b[32m      3\u001b[39m     api_key=OPENAI_API_KEY,\n\u001b[32m      4\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.groq.com/openai/v1\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Groq API ì—”ë“œí¬ì¸íŠ¸\u001b[39;00m\n\u001b[32m      5\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/llama-4-scout-17b-16e-instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     temperature=\u001b[32m0.7\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = model.invoke(\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ê²°ê³¼ ì¶œë ¥\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ”¹ í”„ë¡¬í”„íŠ¸:\u001b[39m\u001b[33m\"\u001b[39m, prompt.format())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:703\u001b[39m, in \u001b[36mBaseChatPromptTemplate.format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    694\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format the chat template into a string.\u001b[39;00m\n\u001b[32m    695\u001b[39m \n\u001b[32m    696\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    701\u001b[39m \u001b[33;03m        formatted string.\u001b[39;00m\n\u001b[32m    702\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.to_string()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:726\u001b[39m, in \u001b[36mBaseChatPromptTemplate.format_prompt\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> PromptValue:\n\u001b[32m    718\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[32m    719\u001b[39m \n\u001b[32m    720\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    724\u001b[39m \u001b[33;03m        PromptValue.\u001b[39;00m\n\u001b[32m    725\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m     messages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1187\u001b[39m, in \u001b[36mChatPromptTemplate.format_messages\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1183\u001b[39m     result.extend([message_template])\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1185\u001b[39m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[32m   1186\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m     message = \u001b[43mmessage_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1188\u001b[39m     result.extend(message)\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:559\u001b[39m, in \u001b[36m_StringImageMessagePromptTemplate.format_messages\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[BaseMessage]:\n\u001b[32m    551\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[32m    552\u001b[39m \n\u001b[32m    553\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m \u001b[33;03m        List of BaseMessages.\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:592\u001b[39m, in \u001b[36m_StringImageMessagePromptTemplate.format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Format the prompt template.\u001b[39;00m\n\u001b[32m    584\u001b[39m \n\u001b[32m    585\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    589\u001b[39m \u001b[33;03m    Formatted message.\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.prompt, StringPromptTemplate):\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._msg_class(\n\u001b[32m    594\u001b[39m         content=text, additional_kwargs=\u001b[38;5;28mself\u001b[39m.additional_kwargs\n\u001b[32m    595\u001b[39m     )\n\u001b[32m    596\u001b[39m content: \u001b[38;5;28mlist\u001b[39m = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\langchain_core\\prompts\\prompt.py:186\u001b[39m, in \u001b[36mPromptTemplate.format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m    A formatted string.\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._merge_partial_and_user_variables(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\string.py:190\u001b[39m, in \u001b[36mFormatter.format\u001b[39m\u001b[34m(self, format_string, *args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\langchain_core\\utils\\formatting.py:33\u001b[39m, in \u001b[36mStrictFormatter.vformat\u001b[39m\u001b[34m(self, format_string, args, kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m     msg = (\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo arguments should be provided, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33meverything should be passed as keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\string.py:194\u001b[39m, in \u001b[36mFormatter.vformat\u001b[39m\u001b[34m(self, format_string, args, kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[32m    193\u001b[39m     used_args = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     result, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_unused_args(used_args, args, kwargs)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\string.py:234\u001b[39m, in \u001b[36mFormatter._vformat\u001b[39m\u001b[34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[39m\n\u001b[32m    230\u001b[39m     auto_arg_index = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m obj, arg_used = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m used_args.add(arg_used)\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\string.py:299\u001b[39m, in \u001b[36mFormatter.get_field\u001b[39m\u001b[34m(self, field_name, args, kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[32m    297\u001b[39m     first, rest = _string.formatter_field_name_split(field_name)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[32m    302\u001b[39m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\string.py:256\u001b[39m, in \u001b[36mFormatter.get_value\u001b[39m\u001b[34m(self, key, args, kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'season'"
     ]
    }
   ],
   "source": [
    "\n",
    "# LLM ëª¨ë¸ ì„¤ì • (GPT-4o-mini ì‚¬ìš©)\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = model.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ”¹ í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ec8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-build-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
