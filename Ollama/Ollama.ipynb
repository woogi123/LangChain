{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e68e21",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "Ollama는 대규모 언어 모델 (LLM Large Language Model)을 로컬 머신 상에서 실행하기 위한 도구입니다.\n",
    "\n",
    "- 로컬 실행 :  Ollama는 대규모 언어 모델을 로컬에서 실행할 수 있게 해줍니다.\n",
    "- 데이터 프라이버시 : 로컬에서 모델을 실행하기 때문에 사용자 데이터를 외부 서버로 전송하지 않기 때문에 민감한 데이터를 처리할 때 장점이 있습니다.\n",
    "- 비용 절감 : 클라우드 서비스 사용에 따른 비용을 절감할 수 있습니다.\n",
    "- 모델 커스터마이징 :  특정 도메인에 특화된 데이터로 모델을 재훈련 하거나 튜닝할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605fa9d5",
   "metadata": {},
   "source": [
    "### Model Distillation\n",
    "> 큰 모델의 지식을 작은 모델로 '전달'하는 기술\n",
    "\n",
    "> 모델의 성능(정확도)을 유지하면서 크기를 줄이는 데 초점\n",
    "\n",
    "큰 모델은 정확도는 높지만 계산 속도가 느리고, 하드웨어 자원을 많이 사용합니다. 작은 모델은 빠르고 경량이지만 정확도가 낮을 수 있습니다. 그래서 큰 모델의 \"지식\"을 전달해서, 작은 모델도 가능한 한 비슷한 성능을 내게 만드는 것이 증류의 목표\n",
    "\n",
    "**진행 방식**\n",
    "   1. 큰 모델(teacher model)은 데이터를 학습해서 높은 정확도로 예측합니다.\n",
    "   2. 작은 모델(student model)이 큰 모델의 예측 결과를 참고하면서 데이터를 학습합니다.\n",
    "       - 작은 모델은 단순히 데이터를 보고 배우는 대신, 큰 모델이 \"이 데이터를 이렇게 봐야 한다\"고 가르쳐 주는 방식."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c9613",
   "metadata": {},
   "source": [
    "### Model Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5549ba5",
   "metadata": {},
   "source": [
    "> 모델이 사용하는 숫자의 정밀도를 낮추어서 처리 속도를 높이고, 메모리 사용량을 줄임\n",
    "\n",
    "AI 모델은 수많은 숫자를 계산해서 정보를 처리하기 때문에 **큰 숫자(32비트, 64비트)로 작업하면 속도가 느려지고, 메모리도 많이 사용**됨. 이런 경우, 정확도를 조금 희생하더라도 **더 작은 숫자(예: 8비트)로 계산**하면 빠르고 효율적\n",
    "\n",
    "Ex. 평소에 온도를 측정할 때 \"1.45272°C\"를 정확히 말하지 않고,  \"1°C\"로 대충 표현해도 충분히 온도를 이해할 수 있음. 이런식으로 정밀도를 살짝 낮춰도 모델이 유용한 결과를 내도록 만드는 과정\n",
    "\n",
    "**진행 방식**\n",
    "1. 기존 모델은 계산에 정밀한 숫자들(예: 32비트 또는 64비트 부동소수점)을 사용합니다.\n",
    "2. 양자화를 통해 이 숫자들을 가볍게 줄이거나 변환합니다(예: 8비트 정수로).\n",
    "3. 결과적으로, 모델의 연산 속도가 빨라지고, 메모리 사용량과 에너지가 줄어듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050cac4",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "> 2개 이상의 model을 하나로 연결하는 것\n",
    "\n",
    "> 각 model의 강점을 활용 가능\n",
    "\n",
    "LangGraph는 다음과 같은 **복잡한 AI 워크플로우**가 필요한 경우 유용.\n",
    "\n",
    "- **멀티 에이전트 협업 시스템**\n",
    "    - 여러 AI 모델(예: 법률 AI, 재무 AI)이 협력하여 답변을 생성해야 할 때\n",
    "    - 예제: 한 AI가 초안을 작성하고, 다른 AI가 검토하는 시스템\n",
    "- **비즈니스 프로세스 자동화**\n",
    "    - LLM이 다양한 분기(Branch)와 조건(Conditional Logic)에 따라 다르게 작동해야 하는 경우\n",
    "    - 예제: 고객 서비스 챗봇이 사용자의 질문 유형에 따라 다른 워크플로우를 실행\n",
    "- **대규모 비동기/병렬 처리 필요**\n",
    "    - 여러 개의 태스크를 동시에 실행하여 응답 속도를 향상 해야 할 때\n",
    "    - 예제: 대량의 문서를 요약하는 작업을 여러 프로세스로 병렬 처리\n",
    "- **RAG(Retrieval-Augmented Generation) 최적화**\n",
    "    - 다양한 검색 전략을 동시에 실행하여 검색 정확도를 높여야 할 때\n",
    "    - 예제: 여러 개의 벡터 저장소에서 동시에 검색을 수행하고 최적의 결과를 선택하는 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0574b9",
   "metadata": {},
   "source": [
    "## Gradio\n",
    "> streamlit AI버전\n",
    "\n",
    "- 파이썬으로 만든 머신러닝이나 데이터 사이언스로 만든 프로그램을 웹 어플리케이션으로 만들어 주는 역할을 합니다.\n",
    "- 사용자가 Web UI를 빠르게 작성할 수 있도록 해주는 Python Open Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c91232",
   "metadata": {},
   "source": [
    "## RAG - Streamlit + deepseek-r1 모델\n",
    "> PDF 업로드 → 문서 분할 → 임베딩 저장 → 검색 → LLM 답변 생성\n",
    "\n",
    "**1. UI 및 스타일링 설정**\n",
    "\n",
    "- `st.markdown()`을 사용해 **Streamlit의 버튼, 입력 필드, 파일 업로더 스타일을 변경**합니다.\n",
    "- `st.title()`을 통해 앱 제목을 설정합니다.\n",
    "\n",
    "**2. PDF 업로드 및 문서 처리**\n",
    "\n",
    "- `st.file_uploader()`를 통해 사용자가 PDF 파일을 업로드하면, 해당 파일을 `temp.pdf`로 저장함\n",
    "- `PDFPlumberLoader`를 사용하여 PDF 문서를 로드하고 **텍스트를 추출**합니다.\n",
    "\n",
    "**3. 문서 분할 및 임베딩**\n",
    "\n",
    "- `SemanticChunker(HuggingFaceEmbeddings())`를 이용해 문서를 **의미론적으로 적절한 크기로 분할함**\n",
    "- `HuggingFaceEmbeddings()`를 사용하여 **텍스트를 벡터 임베딩**으로 변환합니다.\n",
    "\n",
    "**4. 벡터 데이터베이스 (FAISS) 저장**\n",
    "\n",
    "- `FAISS.from_documents()`를 이용해 **FAISS 벡터 스토어를 생성**하고, 문서 임베딩을 저장합니다.\n",
    "- `vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})`를 사용해 **유사도가 높은 3개의 문서를 검색**합니다.\n",
    "\n",
    "**5. LLM 및 프롬프트 설정**\n",
    "\n",
    "- `ChatOllama(model=\"deepseek-r1:7b\")`를 사용하여 **DeepSeek R1 LLM을 로드**합니다.\n",
    "- `ChatPromptTemplate.from_messages()`를 이용해 **질문에 대한 답변을 유도하는 시스템 프롬프트를 설정**합니다.\n",
    "- 답변 생성 시 **최대 3문장으로 간결하게, 반드시 한국어로 응답하도록 설정**합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfc2c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
