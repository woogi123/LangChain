{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85cfe6c",
   "metadata": {},
   "source": [
    "### Ollama 실행체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1654d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d282ea8",
   "metadata": {},
   "source": [
    "### deepseek-r1 과 qwen2.5  모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8260ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, the user asked about LangChain. I know they're a library for working \"\n",
      " 'with language models in Python. Let me break this down.\\n'\n",
      " '\\n'\n",
      " \"First, what is LangChain? It's a comprehensive framework that makes it easy \"\n",
      " 'to work with language models and transformers. So, I should explain its main '\n",
      " 'features.\\n'\n",
      " '\\n'\n",
      " 'I remember it has components like data preprocessors and tokenizers. These '\n",
      " 'are important because they prepare your text for model training or '\n",
      " 'inference. Maybe mention things like text splitting or encoding.\\n'\n",
      " '\\n'\n",
      " \"Then there's the main library itself, LangChainLLM. This is where all the \"\n",
      " 'model types come in handy. It includes models from libraries like '\n",
      " 'HuggingFace or An OpenAI. I should list some popular ones here.\\n'\n",
      " '\\n'\n",
      " 'Model configurations are crucial too. Users might want to set parameters '\n",
      " 'like temperature or max tokens. That would make sense as a feature to '\n",
      " 'mention.\\n'\n",
      " '\\n'\n",
      " 'Preprocessing and tokenizing can be tricky. I should clarify what they do, '\n",
      " 'like splitting text into chunks for multiple models or encoding sequences '\n",
      " 'into tensors suitable for models.\\n'\n",
      " '\\n'\n",
      " 'Evaluation is another key point. Metrics like accuracy, F1 scores, and BLEU '\n",
      " 'are standard in NLP tasks. Highlighting these would help users assess their '\n",
      " 'models effectively.\\n'\n",
      " '\\n'\n",
      " 'I also want to touch on usage examples so the user can see how LangChain '\n",
      " 'fits into their workflow. Starting with data loading, model training, '\n",
      " 'inference, and deployment makes sense.\\n'\n",
      " '\\n'\n",
      " 'Finally, I should summarize why someone might choose LangChain over other '\n",
      " \"libraries. It's modular, built on PyTorch, and versatile for various tasks \"\n",
      " 'like translation or summarization.\\n'\n",
      " '\\n'\n",
      " 'I need to make sure the explanation is clear and covers all bases without '\n",
      " 'getting too technical. Keeping it friendly and reassuring would be good. Let '\n",
      " \"me structure this in a way that's easy to follow step by step.\\n\"\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a Python library designed to simplify working with language '\n",
      " 'models (LMs) and transformers. It provides a flexible and modular framework '\n",
      " 'for tasks such as text generation, interpretation, translation, and more.\\n'\n",
      " '\\n'\n",
      " '### Key Features of LangChain:\\n'\n",
      " '1. **Main Library (`langchain.llm`)**:\\n'\n",
      " '   - Contains various LMs from different libraries (e.g., '\n",
      " '`transformers.huggingface`, `anopen.openai`).\\n'\n",
      " '   - Supports models like GPT, BERT, RoBERTa, etc.\\n'\n",
      " '   - Allows configuration of hyperparameters for tasks such as text '\n",
      " 'generation.\\n'\n",
      " '\\n'\n",
      " '2. **Data Preprocessing**:\\n'\n",
      " '   - Provides tools to preprocess text data before training or inference.\\n'\n",
      " '   - Includes methods for splitting text into chunks (e.g., for multi-model '\n",
      " 'training), tokenization, and encoding sequences.\\n'\n",
      " '\\n'\n",
      " '3. **Tokenizers**:\\n'\n",
      " '   - Offers different tokenization strategies (e.g., `original`, `lower`, '\n",
      " '`upper`).\\n'\n",
      " '   - Supports custom tokenizers for specific needs.\\n'\n",
      " '\\n'\n",
      " '4. **Preprocessing Components**:\\n'\n",
      " '   - Includes text splitters, text reporters, and other components to '\n",
      " 'preprocess text before feeding it into a model.\\n'\n",
      " '   - For example, you might split text into chunks for parallel training or '\n",
      " 'report statistics about the text.\\n'\n",
      " '\\n'\n",
      " '5. **Model Configurations**:\\n'\n",
      " '   - Allows users to configure LMs with specific settings (e.g., '\n",
      " 'temperature, top-k sampling).\\n'\n",
      " '   - This is useful when comparing different models based on specific '\n",
      " 'tasks.\\n'\n",
      " '\\n'\n",
      " '6. **Evaluation Metrics**:\\n'\n",
      " '   - Provides tools for evaluating the performance of language models using '\n",
      " 'standard metrics like accuracy, F1 score, BLEU, and others.\\n'\n",
      " '   - Simplifies the process of assessing how well a model is performing.\\n'\n",
      " '\\n'\n",
      " '7. **Integration with PyTorch**:\\n'\n",
      " '   - LangChain is built on top of PyTorch, allowing seamless integration '\n",
      " 'with other deep learning frameworks and tools.\\n'\n",
      " '   - This makes it versatile for research or production pipelines that '\n",
      " 'require end-to-end processing.\\n'\n",
      " '\\n'\n",
      " '### Usage Examples:\\n'\n",
      " '1. **Data Loading**:\\n'\n",
      " '   ```python\\n'\n",
      " '   from langchain.llm import GPT2LMH\\n'\n",
      " '\\n'\n",
      " '   # Initialize a language model\\n'\n",
      " '   model = GPT2LMH()\\n'\n",
      " '   # Load training data\\n'\n",
      " '  训练数据集, 验证数据集, 测试数据集 = 加载数据集\\n'\n",
      " '   ```\\n'\n",
      " '\\n'\n",
      " '2. **Model Training**:\\n'\n",
      " '   ```python\\n'\n",
      " '   from langchain.llm import GPT2LMH\\n'\n",
      " '\\n'\n",
      " '   # Train a model on a dataset\\n'\n",
      " '   训练模型 = 训练模型(训练数据集, 验证数据集, 测试数据集, 模型, 高温参数=1)\\n'\n",
      " '   ```\\n'\n",
      " '\\n'\n",
      " '3. **Inference**:\\n'\n",
      " '   ```python\\n'\n",
      " '   # Make predictions using the trained model\\n'\n",
      " '   真的文本 = 加载测试数据\\n'\n",
      " '   预测结果 = model.generate(真实的文本, max_tokens=500)\\n'\n",
      " '   ```\\n'\n",
      " '\\n'\n",
      " '4. **Deployment**:\\n'\n",
      " '   ```python\\n'\n",
      " '   # Convert the model to a callable function for deployment\\n'\n",
      " '   from langchain.llm import GPT2LMH\\n'\n",
      " '\\n'\n",
      " '   def deploy_model():\\n'\n",
      " '       return GPT2LMH()\\n'\n",
      " '\\n'\n",
      " '   # Deploy the model using AutoModelForCausalInference.load_from_path()\\n'\n",
      " '   自动模型 loaded = 预训练模型 loading path/\\n'\n",
      " '   应用模型 = 改变部署函数\\n'\n",
      " '   ```\\n'\n",
      " '\\n'\n",
      " '### Why Choose LangChain?\\n'\n",
      " \"- **Modular Design**: It's built to be easy to extend, modular, and \"\n",
      " 'extensible.\\n'\n",
      " '- **Versatility**: It supports a wide range of tasks from text generation to '\n",
      " 'multi-task learning (e.g., cross-model training).\\n'\n",
      " '- **High-Applied Rate**: It helps developers focus on their work while '\n",
      " 'letting machine learning take care of the details.\\n'\n",
      " '- **Integration Capabilities**: Its integration with PyTorch and other '\n",
      " 'libraries makes it accessible for various workflows.\\n'\n",
      " '\\n'\n",
      " 'LangChain is particularly useful for researchers, developers, and anyone '\n",
      " 'working with NLP tasks who want a comprehensive and flexible framework.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e0fe3",
   "metadata": {},
   "source": [
    "### qwen2.5 모델 사용하기 <-> 한국어 지원\n",
    "- 3.0 model: 추론, 2.5 mode: 한국어 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "810afacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and powerful for various tasks. I should mention that it's open-source and has a large community.\n",
      "\n",
      "Wait, the user might be a beginner, so I should explain it in simple terms. Maybe start with the word \"파이썬\" and explain that it's a programming language. Then, mention that it's used for web development, data analysis, scientific computing, etc. Also, note that it's written in Python syntax, which is different from other languages.\n",
      "\n",
      "I should also include some key points like the fact that it's free to use, has a simple syntax, and has a lot of libraries. Maybe add that it's used in fields like data science, machine learning, and automation. Oh, and mention that it's developed by Guido van Rossum. \n",
      "\n",
      "Wait, the user might not know the full scope, so I should keep it concise but informative. Avoid technical jargon unless necessary. Make sure to use clear examples, like how it's used in websites or data analysis. Also, highlight that it's versatile and has a lot of resources available. \n",
      "\n",
      "I should check if there's any common misconception. For example, some might think it's just for beginners, but actually, it's suitable for both beginners and professionals. Also, mention that it's cross-platform, so it works on different operating systems. \n",
      "\n",
      "Okay, structure the answer with a definition, key features, applications, and maybe a note on its popularity. Make sure the response is natural and flows well in Korean. Avoid any markdown and keep the language friendly.\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어로, 간단하고 직관적인 문법을 가지고 있어 빠른 배우기와 다양한 활용이 가능합니다. 이 언어는 웹 개발, 데이터 분석, 사이버 보안, 머신러닝 등 다양한 분야에서 널리 사용되고 있으며, 오픈소스로 무료로 사용할 수 있습니다. 파이썬은 코드를 작성할 때 간단한 구문으로 복잡한 작업도 쉽게 처리할 수 있어, 프로그래머들이 다양한 프로젝트를 개발하는 데 유용합니다. 또한, 파이썬은 데이터 분석과 과학 계산에 강력한 라이브러리(예: NumPy, Pandas, Matplotlib)를 제공해 실용성을 높입니다. 이 언어는 개발자들이 쉽게 배우고, 다양한 분야에서 활용할 수 있는 다능력 있는 프로그래밍 언어로 알려져 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# llama3.2 모델 로드\n",
    "# llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "\n",
    "# qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요? 한국말로\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1491a",
   "metadata": {},
   "source": [
    "### deepseek + qwen 연동 (Lang Graph)\n",
    "- deepseek: 효과좋음 but 한글연동 x\n",
    "- qwen: 효과별로 but 한글 가능\n",
    "-> 이 2개를 연동해 deepseek의 출력을 한글로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085ed4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger between 9.9 and 9.11, I'll start by comparing their whole number parts.\n",
       "\n",
       "Both numbers have the same whole number part of 9, so they are equal in that aspect.\n",
       "\n",
       "Next, I'll compare the decimal parts. Since 9.9 can be written as 9.90, it has a larger decimal value than 9.11.\n",
       "\n",
       "Therefore, 9.9 is greater than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is bigger between \\(9.9\\) and \\(9.11\\), let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Number Part\n",
       "Both numbers have the same whole number part:\n",
       "\\[\n",
       "9 \\quad (\\text{from } 9.9) \\quad \\text{and} \\quad 9 \\quad (\\text{from } 9.11)\n",
       "\\]\n",
       "Since they are equal, we need to compare the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Decimal Parts\n",
       "- \\(9.9\\) can be written as \\(9.90\\) to have two decimal places.\n",
       "- \\(9.11\\) already has two decimal places.\n",
       "\n",
       "Now, compare each digit after the decimal point:\n",
       "\\[\n",
       "9.90 \\quad (\\text{decimal part } 90) \\quad \\text{and} \\quad 9.11 \\quad (\\text{decimal part } 11)\n",
       "\\]\n",
       "- The first digit after the decimal is \\(9\\) in both cases.\n",
       "- The second digit is \\(0\\) for \\(9.90\\) and \\(1\\) for \\(9.11\\).\n",
       "\n",
       "Since \\(0 < 1\\), it means:\n",
       "\\[\n",
       "9.90 < 9.11\n",
       "\\]\n",
       "\n",
       "### Conclusion\n",
       "Therefore, \\(9.11\\) is larger than \\(9.9\\).\n",
       "\n",
       "\\[\n",
       "\\boxed{9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    # print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47a1a916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think about this step by step. \n",
       "\n",
       "First, I need to compare the two numbers. Both are decimals, so I can look at their digits one by one. Let's write them down:\n",
       "\n",
       "9.9 is the same as 9.90, right? Because adding a zero at the end doesn't change the value. Similarly, 9.11 is 9.11. So, comparing them, the first number is 9.90 and the second is 9.11.\n",
       "\n",
       "Now, looking at the whole number part, both are 9. So that's the same. Then, we move to the decimal part. The first number has 9 in the tenths place and 0 in the hundredths place. The second number has 1 in the tenths place and 1 in the hundredths place.\n",
       "\n",
       "So, comparing the tenths place first: 9 vs 1. Since 9 is greater than 1, the first number (9.90) is larger than the second number (9.11). Therefore, 9.9 is bigger than 9.11.\n",
       "\n",
       "Wait, let me double-check. If I think of 9.9 as 9 and 0.9, and 9.11 as 9 and 0.11. So 0.9 is definitely larger than 0.11. So yes, 9.9 is bigger. \n",
       "\n",
       "I don't think there's any trick here. It's straightforward. The decimal parts are compared first, and since 0.9 is greater than 0.11, the whole number part is the same, so the first number is larger. \n",
       "\n",
       "Another way to look at it: if you subtract 9.11 from 9.9, you get 0.79, which is positive, so 9.9 is larger. \n",
       "\n",
       "I think that's solid. No mistakes here.\n",
       "</think>\n",
       "\n",
       "9.9는 9.11보다 더 큰 수입니다.  \n",
       "**증명:**  \n",
       "- 두 수의 정수 부분은 모두 9입니다.  \n",
       "- 소수 부분을 비교하면, 9.9는 0.9를, 9.11은 0.11을 가지고 있습니다.  \n",
       "- 0.9 > 0.11이므로, 9.9 > 9.11입니다.  \n",
       "\n",
       "**답:** 9.9가 더 큰 수입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "# model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    # print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471d177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen2.5:1.5b' temperature=0.7\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문과 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 추론모델 (출력에 think 태그)\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "# 응답모델 (한글처리 가능)\n",
    "generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae6c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.9는 9.11보다 더 크다고 말할 수 있습니다. 이유는 먼저 두 숫자를 비교해보면 같을 뿐만 아니라, 각 소수점 아래의 자리에서 9.90이 9.11보다 크다는 것입니다. 즉, 9.9은 9.90과 비슷하게 표현했지만, 9.11에 더 작은 숫자인 1이 있다 보니 전체적으로 큰 값이 됩니다. 따라서, 9.9는 9.11보다 큰 수입니다.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align them by adding an extra decimal place to 9.9, making it 9.90.\\n\\nNow, both numbers are 9.90 and 9.11.\\n\\nComparing each digit from left to right:\\n\\n- The whole number part is equal (both have 9).\\n- In the tenths place, 9 in 9.90 is greater than 1 in 9.11.\\n  \\nSince 9.90 has a higher value in the tenths place, it is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': '9.9는 9.11보다 더 크다고 말할 수 있습니다. 이유는 먼저 두 숫자를 비교해보면 같을 뿐만 아니라, 각 소수점 아래의 자리에서 9.90이 9.11보다 크다는 것입니다. 즉, 9.9은 9.90과 비슷하게 표현했지만, 9.11에 더 작은 숫자인 1이 있다 보니 전체적으로 큰 값이 됩니다. 따라서, 9.9는 9.11보다 큰 수입니다.'}\n",
      "==> 생성된 답변: \n",
      "\n",
      "9.9는 9.11보다 더 크다고 말할 수 있습니다. 이유는 먼저 두 숫자를 비교해보면 같을 뿐만 아니라, 각 소수점 아래의 자리에서 9.90이 9.11보다 크다는 것입니다. 즉, 9.9은 9.90과 비슷하게 표현했지만, 9.11에 더 작은 숫자인 1이 있다 보니 전체적으로 큰 값이 됩니다. 따라서, 9.9는 9.11보다 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "# LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "# 노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str \n",
    "\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성합니다.\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561644c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG0AAAFNCAIAAACFQXaDAAAAAXNSR0IArs4c6QAAG8tJREFUeJztnXdgU9X+wE920qzukQ7aQqEtbZoOQJDHLkNkK6OALEVARJ4UGTJFnzL04fsJigxFRKk8hlKWVjbUQqGTyurebdpmr3tv8vsjWPsgTdL0pEngfP5K7j333G8/Pffek3vPPV+SwWAAiE5DdnQAzwjIIxyQRzggj3BAHuGAPMKBCqWWulKNUo6rZASBG7RqPZQ67QrDjUyhkNx4FDceLSCU0fkKSZ3pP/55U1ZSqCwtVIbHskkk4MaluvvSdWqi82HZGwaL3NKAqeQ4AKTiAkV4b3ZYDDuqL8/mCm30mHdFknWuubuQExbDDo9h27x7Z8BgAKWFypJCRXG+sv9YL+FAvg2VdNhjfbnm7Ld13eM4A172olBJNuzSacExw/VT4vIi1eg5/r7BHTvYO+bxbqasKEs6doHAjUvpeJyugVJKnD5QEzOAH92vA4d5Bzw+zFVUPVANnepra4SuxO9HGkKj2d2F1p6yrPV481yzXIIPn/5cSDSS8UMD34faJ9nTmsJW9R+L8xVNddrnSiIAYESKb0OltqRQaU1hyx4ljdjDHMWYuQEwYnMxxs4PuJ8tk4pxiyUte7z2i7hXEhdSYK5Hr0Te9VONFotZ8FhbptEoibDert1D7AzhsWyFFK+v0JovZsFjUZZs4ARvqIG5Hv8Y7130h9R8GXMetSp9Sb7CvxsTdmDmSEtL27hxow0bjhgxorq62g4RgYBw1oMcOaY1d9/AnMeSQkVYl//mu3v3rg1bVVVVSSQSO4TzmPAYjvkLt7n+46WjjWEx7G5RbvaIrKSkZM+ePdnZ2RQKRSgUzp49Oy4ubsGCBXl5ecYCR44c6dGjR1pa2tWrVwsLCxkMRlJS0ltvvSUQCAAAqampdDrdz8/v0KFDCxcu/Prrr41bDRs2bNu2bdCjLburKr+nHDzFp90Shvb5YVu5uEZrpoDNaLXa5OTkdevWPXz48N69eytWrBg2bJhGozEYDHPmzNmwYYOxWHZ2dmJi4r59+27dupWZmblgwYL58+cbV61evXrChAlvv/32lStXWlparl69mpiYWFVVZY9oDQZDQ5Xmxx0VZgqYu/+olBF2+h1dXl7e3Nw8Y8aMHj16AAC2bt2ak5OD4ziD8T93B0QiUVpaWmhoKIVCAQBoNJrU1FSFQsHhcCgUSmNjY1pa2hOb2Ak3LlUlM9eLbNejwQA0KoLFsYvHkJAQDw+PDRs2jB07NjExUSgUJiUlPV2MQqFUVlbu2LGjqKhIqXx8empubuZwOACAsLCwrpEIAGBzKSq5ufuq7V5nDHrAYNrrqQODwdi7d+/AgQMPHz48f/78SZMmnTt37uliFy5cSE1NjYuL279/f3Z29s6dO5+oxE7hmYAEaHQSaP9WRLumyBQASECjstdDgtDQ0OXLl6enp+/YsSM8PHzdunUPHjx4osyJEyfi4+MXLVpkPPwVCoWdgrGIWkFQ6WTQ/u1Wcy3O4knBZkpLS0+dOgUAYDKZQ4YM2bp1K5lMvnfv3hPFpFKpj8/fl8gLFy7YIxhrsHipMOdREM5SK+zysKWlpWXz5s07d+6sqqoqKSk5cOCAXq8XCoUAgODg4KKiouzs7JaWlp49e968efPOnTs4jn///ffGq01dXd3TFYaGhgIAMjIybOt+WkQtJwLCWGYKmPPoE0h/kCO3Q1QgISFh7dq1Z8+enThx4tSpU/Pz8/fs2WN0MXnyZIPBsGTJkuLi4qVLl/bt23f58uX9+/cXi8WbNm3q1avXkiVLnm6YQUFB48aN+/LLL3ft2mWPgB/myi08aTDTJ1LK8P0bSuzQG3M99q4rVitwMwXMnx8pQT3dxNUWbnU88zRU6kKj2Ey2ufOjhXEAkYncG+lN498UtFdg0aJFT18fAAA4jgMAqFTT9aenpxv7gNDJz89ftmyZyVU4jrcXDwDg4sWLJJLp6/GN9MakERaeLlh+PnNiV3XfUZ6BPUyfZRsbGzEMM7lKq9W218Uz/ka2EzU1NTZs1V5IlQ/Ut39vnrg40Pzmlj02VGjzr0tHzHi+Hs60knG4XjTY3TvIQp/f8i8W3xCGfzfGxaMN8GJzGS6kNQh6sCxKtPZ5YcwAPplMyjzdBCM2l+H6KTGNQbZyNEAHxgHkXZGoFfoXXrLqea6rcyO9ietOjbV6rE8H7kTEDXInU8HpA7W2xuYaGAwgfV8NnUm2XqIt46RKCpXnvq3tN8YrcbhHx4N0drJ/a8nOaB79mn9oBx+R2jhuL/N0U1GWLLofL6w32z+0Sx+E2YPaMk1pofJupjT2Rf4LL3nZUIPt40h1an3BdWnpXaWkURceyyVTAJtH4XvRcMwFXmyi0klSMaaUEXrCUFyg8PClh/VmCwe60xg2jkTs1HhcIxqlvrZUo5BiKhlhMACVHPKttvPnz48aNQpunW48CgmQ3HgUjjstIIzJdOvsHWsIHu1Nnz59bt265egoLIDeV4AD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuHgAh75fFsmeOpiXMCjVGrhXXxnwAU8ugTIIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEg/O+hxQfH08ikUikxxEaJ4+4ffu2o+MyjfO2R4FAQCaTSSQSmUw2fggIcN45o53XY3x8fNtjhSAI44RTzonzekxJSfH392/9GhgYOGvWLIdGZA7n9RgdHR0fH9/6VSQSRUdHOzQiczivRwDA9OnTjU3S399/5syZjg7HHE7tMSYmxnhOTEhIiIqKcnQ45oCTn8uIQQ9qStWSBkyjgjbb4cCY12QV3v2jxt7+vQVWnUw3iocvLSCMRYLXiqD1H2tLNdd+EZMAKaC7G252ynKHQ6WTa0qUAIB/TPSGNcs8HI8NldrLxxtHzAyk0lwm0xSuM2T8UD14io+vFdNFWQRCy9aq9Ce/rB49N8iFJBqn+hg9N+jEF1XmJ/y3EggeszNaEoa7ai6LhOHe2RkQzrwQPNaVq919aJ2vxyHwfeh1ZZrO1wPjuFbqWTyY1/2uhM2jqpUQehcQPBJ6g5kJyp0cgwHoCQjRO3U/3IVAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCwfEeX502Zt9+08l3xk0YcviHb8xvfuz4keHJfe0TWgdwjMdNm1edOfuzxWLTp82JjRF1RUCdxjEe7923KovWzJR5QmG8FQUdT1d71Ov1Q4cn1dfXbd+xZcKk4caFVCrt+PEjyaNeeHn84DXvL5fJZcblrcf1sWM/Tnl1VHl56Zx5rwwdnrTgjennz6c/XTlBEKkrl8x6bZJW29U5nLraI5lMPnfmOgBgZer6n0/8blx48dKvao1629YvUlesz8u7/e3BPU9sRaPT5XLZ5//Zuvq9TRcybg18ccj2T7eIxU+mSd+244NHxQ+2bf2iS1NEAgD5+bXNcDjcmSnzjJ+vXbtYkJ/zRAEymYxh2Ly5i6KiYgAAI0e+/N2hfY8e3ff2/ju74cHv9l68+OvnO/cJAizkLrIHjr9eAwDaXkx4fHetzvRRGRnZ2/iBy+UBABRKhXFcJIlEyvj93LcH96xdsyXqrzJdjFN4bJt+rL1kY+2tMhgMBEF8snWjsV3bLUYLOIXHzrPi3fdHjhz78ScbJBJow1c6xLPgkUwmjxk9fvmy1UwGc+v2zY6Joet3yWAwfHx879y5mZObbUxzCAUWi7V2zZasrOvHT6TBqtN6HNMeZ6bMz76dtX7DCp1OB7Ha3r2Fr81+fc/Xn7e0NEOs1hogjJM69K/yYTMEPE+XHFIhFWOXfqqZtaZbJ+t5Fs6PzgDyCAfkEQ7IIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4QPDI9aLhWld9YQHT6fleEO5UQfDI96A2Vqs7X49DEFdpeE7iMWaAe0mBvPP1OISSAnnMAAjzakPw6BNEFw7kX/lvXeer6mIuH60TDXb3CqB3vipo718X3pAVFyjZfKpvCAvKG1L2g0wmNVSoFRK8ZwI7uh8PSp0w50GSNGAV91XyFlwpg5naPjc3TySKg1ghm0flelK7RbrxvaE9C3He+aRaQXntnyOQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAcX8Ojt7QKTaruAR7FY7OgQLOMCHl0C5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wsF530MSiUQUCsU446hxMlK9Xp+T8+TUuU6C87ZHgUBgnPu2Na99UFCQo4NqF+f1KBKJ9Pq/M4YSBBEbG+vQiMzhvB6nT58uEAhavwYFBaWkpDg0InM4r0ehUNi2AQqFwpiYGEcGZBbn9QgASElJ8fX1Nea1nzFjhqPDMYdTe4yNjTWms4+Pj3fmxmhVXoCWBkxcrVXKYb6abj3D+yxQ1Hi/GDsp94rEIQFweFRvAcPd18Ib72b7jwaQfqBW3ozzfegMFgV+jK6ARknIm3U8L+pL8wLMFGvXo14Pjn9RHdXPPSSSbbcgXYbyIsX9bOnkpYHtZS1o1+PJr2oi+7gH9nCzb4CuQ9UD1cMcyfiFApNrTV9naks1JBIJSWxLUE83gx7Ul5tO3m7ao7hG68Z1itQ0TgWLQxXXmp6A37RHtZxg85HHJ2HzqSqp6X6LaY+wsr0/Y+j1oD0pTt0PdyGQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7h8Ix73LR51ZmzP3fBjp5xj/fu3+2aHZl+rpB1thnDQNxgT+sramoSb9226W5RfkhI2KQJU0vLim/eurF/7xEAgFjcuPvLz+4W5Wu12r59B8x5bWGgIAgA8OjRgzfeTNm96+DhHw5cv37Z19dv6JCRby5cZszPXFCQe/C7r+/fL/L08n6h38C5c95ksVgAgP8e++FI2nfL31m9afOqyZOmL1n8z8zMqxcuns/Lv6NQyKMiY2bPel0kSsRxPHnUC8bYeDy+Mff7mbM/n0o/XlZWHB4eMWzoqCmTp3dIVu6lZgYT9B1lQgu09rht++bKyvJPd3z1wabt165fun07y6gDx/F3UxcVFOamrlj/zf6fuFze4sWza+tqAAB0Oh0AsOPTLckjXvr1XObqVZvTfjp06XIGAKCiouy91UsxHNu96+DG9Z88fHjv3dRFxuE+NBpdrVYdSftu7Zot48e/olKpPvzX+ziOr1n9wUcf/jswMPj99f+USFqoVOq5M9cBACtT1xsl/vbbme07tkT2iv7x8Kl5cxf9dPTQ7i//DevPh+OxqUl881bm9OlzIntF+/j4rnj3/ZraKuOqvPw7lZXla1Z/0CfpBQ8Pz7cWv8vhcI8d+9GYbxkAMGRw8uBBw2k0Wrwoyc/P/8GDPwEAGb+fpVFpH2zaHhzcLTy8x4oV6+7du3sj8woAgEKhqFSqBfOXDBs6Migw2M3Nbd/eI8vfWR0vSooXJS18Y5lKpSoszHs6yFOnjwuF8e8sW+Xu7pGU2G/OawuPnzgik8ugGIDjsbSsuG16ej7fXSRKMn4uKMil0WgJ8X0e749MFsYlFBT8PYyxZ8+o1s8cDlehkAMACgvzIiN78/nuxuWBgiB/v4C8vDutJXv1jG79rFIq//N/216ZOnro8KRxE4YAACTSJ7OJ4zheVFTQJ6l/65L4+D4EQRj/bZ0HzkMYpVIBAGCyWK1LeFx+XV0NAEChkGMYNnR4UtvyXl5/v+JvbJVPoFDIHz66/8RWLS1NrZ+N5wQAQF1d7Tv/fL1PUv8N6z6Ojo4lCGL0Sy8+XaFGoyEIYv+B3fsP7G67XCqFM0wDjkcGnQEAINokBW+RPM5A7eXlzWKxPvrwf85EVIqF/Xp6eceyWPPmLmq7kM9zf7rkhYvnMQxb9d4mJpNpxguHw2EymaNHjRs0aHjb5SHBoVb8fZaB41EgCDIe3cHB3QAAMrksNzc7MDAYABAeHqFWq/39BQH+j5+gV9dUeXp4ma+we3jExYu/iuISSX8NYCgrKwkKCnm6pFQq4XJ5RokAAONlyiTh4RFqjTr+rxOOTqerr69te2R0Bjjnx5CQ0ODgbt8e3FNTWy1XyHfu/NhoFgDQr++Avn0HbN/+QX19nUTScvxE2qJFs87/mm6+wqlTZ+ME/sXuTzUaTUVF2Vd7Pp//+rTy8tKnS/bo3rOpSXz6zEkcx//Iul5YmMthcxoa6gAADAbDx8f3zp2bObnZOI6/+cayK1d+P3P2Z4Ig8vNzNm9ZvWLlYgzDoBiA1u9ZtXKjXq+fNXtiauri3tHCqMgYGvXxGK2PP9o5aNDwDz5cM2lK8s+/HB0zZsLECa+ar43P4+/fl8ZkMF9fOGPOvFfy8u+sWrmxe/eIp0uOGDFmZsq8b779KnnUCydOpr29dGXyyLGHvt//f7t2AABmpszPvp21fsMKnU4nFMbv+fL7/PycSZNHvLd6qVql+nDLZzQanNQp0PrhUqlEo9H4+fkbv763aimbzdm44RMoUToJXdEPX78x9d0Vb167dqmlpfngd3tzcrNffnkyrMqdH2jtUSJp2f7plvLy0qamxm4hYXNeW9i//z+ghup4zLRHaIN43N09PtryGazaXI5n/H5Pl4E8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuGAPMLBtEcm+zl9m9ACBsBqx4xpj57+9IYKV01Vbz/qK9Se/qaTjpv2GBzB0qj1KqhprF0dpRTHdPrA7iyTa9s5P5LAmDn+V0/U6zR60wWeM7Qq/bWT9S/N9Qcdfd8VACBpxH76d2X3OB7fm85we06vSFoFIW3WlRTIpy4PNpO/3fI8SEV/yBurtXBT1XeIoqKi6OhoKwraBTaP4hPEiO7HM1/MeeeTagXltX+OQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMcXMCjv7+/o0OwjAt4rKurc3QIlnEBjy4B8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4eC87yElJCQY09kbp4A0GAwGg+HOnTtWbOoAnLc9BgQEGNPZG7+SSKTAwEBHB9UuzutRKBS2PVb0er0D3zK0iPN6nDZtWtu89oGBgSivvS2IRKLIyMjWr0KhMC4uzqERmcN5PQIAZs6c6eXlBQDw8fGZNm2ao8Mxh1N7FIlExnT2MTExQqHQ0eGYA2YyXJWMUMlxpYzQqvQ6LQGlzuR+82VV/OF9phTekEKpkM4gM9wobB6FzaeyONCmhYHQf2yo0BYXKB/lKcg0qlaJUxkUOpuux5y0W0qmkXRKHa4jGG5UPY5HxHHCYth+IYxOVtspj/Xlmisnmgg9icJkcL3dmFzTc7I4LRq5Ti5W6bU6CkU/aKK3byds2u7xt8MNteVar1BPtgfT5t07CYpmTVNZsyCckTzD17YabPGokODff1IR1NuX4216MhsXRSFWVxc1zFrdjc3v8Hmzwx6lzfhPn1WG9wuiUJ36Wm8bBKYvzqqanhrM8+jYFbhjHsU12lP7GsL6CKwo68KU3qoev9Dfq50puEzSgTZlMIAjOyqfeYkAgLA+gT9uq+jQJh1oj8e+qOX4ezLYMLucTotWiSnrWya/FWBleWvbY+5liQ6jPCcSAQAMNk2jJeddtbbzb63HzNNNfhEdSLfwDOAX4Zl5usmKgsBajzmXJP4RnmRKO3PNPaNQqGT/7u55l61qklZ5LMyUsdydt7N99OePP901yx41M/iswj8geZQ141q1nslxsd98UGBx6So5oZBYnmvQssfyP5Xu/hxIgbkeHgJu2Z9Ki8UsX38bKrVkmh0bY9btX7KyT9bVFwf4R4hik//R//H92vUfjRiTvFgub/rt0n4mg90rov+El97lcb0AAFqt6vB/NzwqyQ7w6/Fiv1fsFxsAgESlNFbqQH8LxSy3R4WUoDLsNX3z7dyzR09+FCSIWrvi5KhhCy9fP/zL2c+Nq2g0xoUr39FojC1rM1YuSyspy/nt0n7jqp9OfiRuqlw8f/ecGVurax88ePSHncIDANAYVDmU41opxWl28/hH9snwbvGTx63ksD169uibPPT1a3+kKZXGXI4kX++QYYPmsFhcPs+nZ/e+1TX3AQBSWWNeYcbQgbODA6N5XK+XR71NpdjxcKEyKNbMxWrZI5VOIVPs4pEg8PLKgp4R/VqXRIQn6fVEafnjLLdBgX+nfmWxeGqNHADQ3FINAPDzDTMuJ5FIQYLIp+qGBplCptIs//mWz48UigHTYPb4JaPDNHo9cS7jq3MZX7VdLlc2//XRRI9VqZICAJiMvy99dLodb99hGpxqRYpDy3bYfKoG0sOWJ2AxOXQaMyn+ZWHvYW2Xe3sFmYvHjQ8AwHBt6xKN1vL11GZwLc7mW7ZkuYR3IKOi2F6ziAf4R+gwdY/wRONXDNe1tNS68/3MbOLhLgAAlFcWBAb0BADodJpHJdk8no+dItQTBm+B5fOv5fNjYHemrEEBKaonGTvyrfy7F7Ju/0IQRElZzqG0tXu+XYrhOjObuPN9Q0PizmV8JW6qxDDt4aPrSaYyP8NC1qBobw77tlhujwGhTK0SIzA9hQY/3PDQ+OWLDl64cjD93H9wQhcSFDNv5nYa1cL/f8aUjcdObf1s1yycwPomjE8Sjb3/MBN6bAAAXEdgGtyap4lW3X+8fLxJKqPx/NiQwnMZJLVKTw9s0CQLWaatvU8RP4TfUNxsRcFnjcaSpoShfGtKWtWb4XlSQ6PdmqvknkFckwVu3Dx25rfdJlcRBEahmO44pEzZHB050JoArOHSte8zLn9jchWLyVNrZCZXzZ/1aXg3kclVTZWy7rEcjrtViqx9rqBV6Y/trhX0Nj3FAYbrcExrcpUO09Bppu+50eksiqUE99aDYVq8nQsUjmPUdjqBZmKoKax75e0AOtOqQ7YDz2dK7yqvnZIEx7nAbBGdpyK3dvAkz26RblaW78AlOKw3u1eCW919sa2xuQy198TRfdjWS7RlHEBhpjw/UyWI8u54eK5BzZ/iuBfZvft17JZrh7uEMf25veLolXkuMIeJDVTm1UbGMzoq0fZxUhX31ZeOiTnebM9gq7oFzk9ThVTZpBj2qk9QhC13PWwfb6bHwfV0cVGWzDvUg+PFYrCtuCvifGgVmKJF3VjSEtOfP2Ccl82/MDs7jlSjJHIuSR/ckWOYge/HNQBAY1BoTBoATjqOFJAApsYxLQEAkNXJaQxSr0Ru/GD3TiYgg/Y+l1SM1ZRomut1Cilh0AOFBINSLXQ47jQSGXD4FE8/uiCcaSZ1WYdw3vfiXItncAyjQ0Ae4YA8wgF5hAPyCAfkEQ7IIxz+HxDUFTTxwYFRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a6219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-build-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
