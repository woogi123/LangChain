{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85cfe6c",
   "metadata": {},
   "source": [
    "### Ollama 실행체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1654d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d282ea8",
   "metadata": {},
   "source": [
    "### deepseek-r1 과 qwen2.5  모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8260ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, so I'm trying to understand what LangChain is. From what I see, it's a \"\n",
      " 'library or tool related to language models. The user provided some code '\n",
      " 'snippets that show how to use the LangChain package with an example model '\n",
      " 'called GPT-3.2.xxxx.\\n'\n",
      " '\\n'\n",
      " \"First, I'll look at the imports. They import LangChain as lc, which makes \"\n",
      " \"sense because the name suggests it's for handling language chains. Then \"\n",
      " \"there's a setup function where they import numpy and pandas, which are \"\n",
      " 'common libraries for data manipulation and analysis.\\n'\n",
      " '\\n'\n",
      " \"In the example, they create a sample DataFrame with three columns: 'user', \"\n",
      " \"'thought', and 'response'. That looks like a typical dataset setup—rows \"\n",
      " 'represent user comments, their thoughts about them, and what they said. The '\n",
      " 'response column is filled with \"Yes\" and \"No\", which might indicate whether '\n",
      " 'the language model agreed or disagreed.\\n'\n",
      " '\\n'\n",
      " \"They then set up an instance of the GPT-3 model from LangChain's models \"\n",
      " 'module. So, the library includes various model instances, probably for '\n",
      " 'different architectures or specific uses. They also import pandas and numpy '\n",
      " 'as data and setup variables to hold the user thought vectors and response '\n",
      " 'outputs.\\n'\n",
      " '\\n'\n",
      " 'The main function trains the model with the provided data. That makes sense '\n",
      " 'because training a language model is a typical use case. After training, '\n",
      " 'they evaluate the model using accuracy and F1 score, which are common '\n",
      " 'performance metrics in machine learning.\\n'\n",
      " '\\n'\n",
      " \"Now, I'm thinking about what LangChain is exactly. It's probably an \"\n",
      " 'open-source Python library designed for handling language modeling tasks. It '\n",
      " 'might include utilities for data loading, preprocessing, model training, '\n",
      " 'evaluation, and inference. The setup function seems to provide a basic '\n",
      " 'structure that other libraries can build upon.\\n'\n",
      " '\\n'\n",
      " 'I wonder if there are more functions or classes in LangChain that handle '\n",
      " 'specific aspects like text generation, summarization, or multilingual '\n",
      " 'support. Since the example uses GPT-3, maybe LangChain is meant for training '\n",
      " \"models on language-focused datasets. Also, I should check if it's compatible \"\n",
      " 'with other libraries like PyTorch or TensorFlow, given that GPTs are trained '\n",
      " 'in those frameworks.\\n'\n",
      " '\\n'\n",
      " \"I'm also curious about how to use different model architectures within \"\n",
      " 'LangChain. For example, could they train a CNN or RNN instead of the default '\n",
      " \"GPT-3? That might expand the library's capabilities beyond just language \"\n",
      " \"models. But without more information, it's hard to say.\\n\"\n",
      " '\\n'\n",
      " 'Testing is another aspect—maybe users can try running the setup function and '\n",
      " 'see how well the model performs on their own datasets. If LangChain is '\n",
      " 'widely used or has a community around it, it could have good documentation '\n",
      " 'and resources available.\\n'\n",
      " '\\n'\n",
      " 'Overall, I think LangChain provides a modular and versatile framework for '\n",
      " 'developers working with language models, especially those focused on text '\n",
      " 'generation and analysis. It simplifies the setup process by handling common '\n",
      " 'tasks like data loading, model training, evaluation, and inference.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is an open-source Python library designed to simplify the '\n",
      " 'development and deployment of language models, particularly focusing on text '\n",
      " \"generation and analysis. Here's a concise overview:\\n\"\n",
      " '\\n'\n",
      " '1. **Purpose**: LangChain is tailored for training and deploying language '\n",
      " 'models on datasets, offering a user-friendly interface.\\n'\n",
      " '\\n'\n",
      " '2. **Structure**: It includes modules for data manipulation, model selection '\n",
      " '(like GPT-3), and utility functions for preprocessing and evaluation.\\n'\n",
      " '\\n'\n",
      " '3. **Key Features**:\\n'\n",
      " '   - **Setup Function**: Provides a basic structure for managing and '\n",
      " 'analyzing datasets.\\n'\n",
      " '   - **Model Management**: Includes instances of various models, such as '\n",
      " 'GPT-3, allowing flexibility in training architectures.\\n'\n",
      " '   - **Data Handling**: Offers tools for loading, preprocessing, and '\n",
      " 'splitting data into training and testing sets.\\n'\n",
      " '\\n'\n",
      " '4. **Example Usage**:\\n'\n",
      " '   - A sample dataset with user thoughts and their responses is created.\\n'\n",
      " '   - The GPT-3 model from LangChain is trained using this data.\\n'\n",
      " '   - Metrics like accuracy and F1 score are used to assess model '\n",
      " 'performance.\\n'\n",
      " '\\n'\n",
      " '5. **Community and Resources**: LangChain has a community, and there are '\n",
      " 'open-source resources available for learning and contributing.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain streamlines the development process for language '\n",
      " 'models, making it accessible for researchers and developers.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e0fe3",
   "metadata": {},
   "source": [
    "### qwen2.5 모델 사용하기 <-> 한국어 지원\n",
    "- 3.0 model: 추론, 2.5 mode: 한국어 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810afacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is in Korean. Let me start by recalling the basic definition of Python. Python is a high-level programming language known for its readability and simplicity. It was created by Guido van Rossum in 1991. I should mention that it's widely used in web development, data analysis, artificial intelligence, and more.\n",
      "\n",
      "Wait, the user might be a beginner, so I need to keep the explanation straightforward. Maybe start with the main points: it's a general-purpose language, easy to learn, and has a vast ecosystem. Also, note that it's open-source and has a large community.\n",
      "\n",
      "I should also highlight some key features like dynamic typing, garbage collection, and a syntax that's easy to read. Maybe mention that it's interpreted, which means you don't need a compiler. Oh, and that it's used in various fields like data science, machine learning, and automation.\n",
      "\n",
      "Are there any common misconceptions? Maybe that it's only for beginners, but actually, it's suitable for both beginners and professionals. Also, the fact that it's cross-platform. Should I include examples? Like, \"Python is used in Django for web apps\" or \"Pandas for data analysis.\" But the user might not need examples, just the definition.\n",
      "\n",
      "Check if there's any recent update or change in the language. Python 3.11 was released in 2023, but the basics are still the same. No need to mention that unless the user asks.\n",
      "\n",
      "Make sure the answer is concise but covers the essentials. Avoid technical jargon unless necessary. Alright, time to put it all together in Korean.\n",
      "</think>\n",
      "\n",
      "파이썬은 고급 언어로, 코드의 읽기 쉬움과 간결함을 특징으로 하는 프로그래밍 언어입니다. 1991년에 제작된 Guido van Rossum이 만든 것이며, 데이터 분석, 웹 개발, 인공지능(AI), 자동화 등 다양한 분야에서 널리 사용됩니다. 코드는 명확하고 직관적이며, 동적 타입과 간단한 문법으로 초보자도 쉽게 배우고 사용할 수 있습니다. 또한, 오픈소스이며 커뮤니티가 활발하여 빠르게 발전하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# llama3.2 모델 로드\n",
    "# llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "\n",
    "# qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요? 한국말로\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1491a",
   "metadata": {},
   "source": [
    "### deepseek + qwen 연동 (Lang Graph)\n",
    "- deepseek: 효과좋음 but 한글연동 x\n",
    "- qwen: 효과별로 but 한글 가능\n",
    "-> 이 2개를 연동해 deepseek의 출력을 한글로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085ed4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I need to compare the two numbers: 9.9 and 9.11.\n",
       "\n",
       "Both numbers have the same whole number part, which is 9.\n",
       "\n",
       "To make a fair comparison, it's helpful to align their decimal places by writing 9.9 as 9.90.\n",
       "\n",
       "Now, comparing the tenths place, 9 in both cases are equal.\n",
       "\n",
       "Next, compare the hundredths place: 0 for 9.90 and 1 for 9.11.\n",
       "\n",
       "Since 0 is less than 1, 9.90 (which is equivalent to 9.9) is smaller than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, follow these steps:\n",
       "\n",
       "1. **Align the Decimal Places:**\n",
       "\n",
       "   To make an accurate comparison, it's helpful to write both numbers with the same number of decimal places.\n",
       "\n",
       "   \\[\n",
       "   9.9 = 9.90\n",
       "   \\]\n",
       "   \n",
       "2. **Compare the Whole Numbers:**\n",
       "\n",
       "   Both numbers have the same whole number part (\\(9\\)).\n",
       "\n",
       "3. **Compare the Tenths Place:**\n",
       "\n",
       "   - The tenths place in \\(9.90\\) is **9**.\n",
       "   - The tenths place in \\(9.11\\) is also **9**.\n",
       "   \n",
       "   Since they are equal, move to the next decimal place.\n",
       "\n",
       "4. **Compare the Hundredths Place:**\n",
       "\n",
       "   - The hundredths place in \\(9.90\\) is **0**.\n",
       "   - The hundredths place in \\(9.11\\) is **1**.\n",
       "   \n",
       "   Here, **0 < 1**, which means \\(9.90\\) is less than \\(9.11\\).\n",
       "\n",
       "5. **Conclusion:**\n",
       "\n",
       "   Therefore, **\\(9.11\\) is larger than \\(9.9\\)**.\n",
       "\n",
       "\\[\n",
       "\\boxed{9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    # print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a1a916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's see. The question is asking which is bigger between 9.9 and 9.11. Hmm, both numbers are decimals. Let me think about how to compare them.\n",
       "\n",
       "First, I know that when comparing decimals, you start by looking at the digits from left to right. So, let's write them out: 9.9 and 9.11. Both have the same whole number part, which is 9. So, the whole numbers are equal. That means we need to look at the decimal parts.\n",
       "\n",
       "The first number, 9.9, has a decimal part of 0.9. The second number, 9.11, has a decimal part of 0.11. Now, I need to compare these two decimal parts. \n",
       "\n",
       "I remember that 0.9 is the same as 0.90, right? So, if I convert 9.11 to have the same number of decimal places as 9.9, it would be 9.110. Then, comparing 9.900 and 9.110. \n",
       "\n",
       "Wait, but 0.900 is larger than 0.110 because 0.9 is more than 0.1. So, even though the whole numbers are the same, the decimal parts are different. Therefore, 9.9 is larger than 9.11.\n",
       "\n",
       "But let me double-check. Maybe I should break it down step by step. \n",
       "\n",
       "First, compare the integer parts. Both are 9, so they are equal. Then, compare the tenths place. The first number has 9 in the tenths place, and the second number has 1 in the tenths place. Since 9 is greater than 1, the first number is larger. \n",
       "\n",
       "So, even without converting to the same decimal places, the tenths place already tells us the difference. The tenths digit of 9.9 is 9, and for 9.11 it's 1. So, 9.9 is bigger. \n",
       "\n",
       "I think that's right. There's no need to go further because once the tenths place is different, the rest of the digits don't matter. \n",
       "\n",
       "Another way to think about it: 9.9 is the same as 9.90, and 9.11 is 9.11. Comparing 9.90 and 9.11, since 9.90 is larger than 9.11, 9.9 is bigger. \n",
       "\n",
       "Yeah, I'm pretty confident now. The answer should be 9.9.\n",
       "</think>\n",
       "\n",
       "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
       "\n",
       "**이유:**  \n",
       "1. 두 수의 정수 부분은 모두 9입니다.  \n",
       "2. 소수 부분을 비교할 때, 9.9는 0.9, 9.11은 0.11입니다.  \n",
       "3. 0.9는 0.11보다 크므로, 9.9가 더 큰 수입니다.  \n",
       "\n",
       "따라서, **9.9**이 더 큰 수입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "# model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    # print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a471d177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen2.5:1.5b' temperature=0.5\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문과 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 추론모델 (출력에 think 태그)\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "# 응답모델 (한글처리 가능)\n",
    "generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ae6c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're absolutely right! Let's break down why 9.9 is indeed greater than 9.11:\n",
      "\n",
      "First, let's align the numbers by adding an extra decimal place to 9.9:\n",
      "- 9.9 becomes 9.90\n",
      "\n",
      "Now we can compare them easily:\n",
      "- Both have a whole number part of 9.\n",
      "- In the tenths place: 9 in 9.90 is greater than 1 in 9.11.\n",
      "\n",
      "So, 9.90 (which is 9.9) is clearly larger than 9.11. Therefore, 9.9 > 9.11.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align them by adding an extra decimal place to 9.9, making it 9.90.\\n\\nNow, both numbers are 9.90 and 9.11.\\n\\nComparing each digit from left to right:\\n\\n- The whole number part is equal (both have 9).\\n- In the tenths place, 9 in 9.90 is greater than 1 in 9.11.\\n  \\nSince 9.90 has a higher value in the tenths place, it is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"You're absolutely right! Let's break down why 9.9 is indeed greater than 9.11:\\n\\nFirst, let's align the numbers by adding an extra decimal place to 9.9:\\n- 9.9 becomes 9.90\\n\\nNow we can compare them easily:\\n- Both have a whole number part of 9.\\n- In the tenths place: 9 in 9.90 is greater than 1 in 9.11.\\n\\nSo, 9.90 (which is 9.9) is clearly larger than 9.11. Therefore, 9.9 > 9.11.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "You're absolutely right! Let's break down why 9.9 is indeed greater than 9.11:\n",
      "\n",
      "First, let's align the numbers by adding an extra decimal place to 9.9:\n",
      "- 9.9 becomes 9.90\n",
      "\n",
      "Now we can compare them easily:\n",
      "- Both have a whole number part of 9.\n",
      "- In the tenths place: 9 in 9.90 is greater than 1 in 9.11.\n",
      "\n",
      "So, 9.90 (which is 9.9) is clearly larger than 9.11. Therefore, 9.9 > 9.11.\n"
     ]
    }
   ],
   "source": [
    "# LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "# 노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str \n",
    "\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성합니다.\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561644c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG0AAAFNCAIAAACFQXaDAAAAAXNSR0IArs4c6QAAG8tJREFUeJztnXdgU9X+wE920qzukQ7aQqEtbZoOQJDHLkNkK6OALEVARJ4UGTJFnzL04fsJigxFRKk8hlKWVjbUQqGTyurebdpmr3tv8vsjWPsgTdL0pEngfP5K7j333G8/Pffek3vPPV+SwWAAiE5DdnQAzwjIIxyQRzggj3BAHuGAPMKBCqWWulKNUo6rZASBG7RqPZQ67QrDjUyhkNx4FDceLSCU0fkKSZ3pP/55U1ZSqCwtVIbHskkk4MaluvvSdWqi82HZGwaL3NKAqeQ4AKTiAkV4b3ZYDDuqL8/mCm30mHdFknWuubuQExbDDo9h27x7Z8BgAKWFypJCRXG+sv9YL+FAvg2VdNhjfbnm7Ld13eM4A172olBJNuzSacExw/VT4vIi1eg5/r7BHTvYO+bxbqasKEs6doHAjUvpeJyugVJKnD5QEzOAH92vA4d5Bzw+zFVUPVANnepra4SuxO9HGkKj2d2F1p6yrPV481yzXIIPn/5cSDSS8UMD34faJ9nTmsJW9R+L8xVNddrnSiIAYESKb0OltqRQaU1hyx4ljdjDHMWYuQEwYnMxxs4PuJ8tk4pxiyUte7z2i7hXEhdSYK5Hr0Te9VONFotZ8FhbptEoibDert1D7AzhsWyFFK+v0JovZsFjUZZs4ARvqIG5Hv8Y7130h9R8GXMetSp9Sb7CvxsTdmDmSEtL27hxow0bjhgxorq62g4RgYBw1oMcOaY1d9/AnMeSQkVYl//mu3v3rg1bVVVVSSQSO4TzmPAYjvkLt7n+46WjjWEx7G5RbvaIrKSkZM+ePdnZ2RQKRSgUzp49Oy4ubsGCBXl5ecYCR44c6dGjR1pa2tWrVwsLCxkMRlJS0ltvvSUQCAAAqampdDrdz8/v0KFDCxcu/Prrr41bDRs2bNu2bdCjLburKr+nHDzFp90Shvb5YVu5uEZrpoDNaLXa5OTkdevWPXz48N69eytWrBg2bJhGozEYDHPmzNmwYYOxWHZ2dmJi4r59+27dupWZmblgwYL58+cbV61evXrChAlvv/32lStXWlparl69mpiYWFVVZY9oDQZDQ5Xmxx0VZgqYu/+olBF2+h1dXl7e3Nw8Y8aMHj16AAC2bt2ak5OD4ziD8T93B0QiUVpaWmhoKIVCAQBoNJrU1FSFQsHhcCgUSmNjY1pa2hOb2Ak3LlUlM9eLbNejwQA0KoLFsYvHkJAQDw+PDRs2jB07NjExUSgUJiUlPV2MQqFUVlbu2LGjqKhIqXx8empubuZwOACAsLCwrpEIAGBzKSq5ufuq7V5nDHrAYNrrqQODwdi7d+/AgQMPHz48f/78SZMmnTt37uliFy5cSE1NjYuL279/f3Z29s6dO5+oxE7hmYAEaHQSaP9WRLumyBQASECjstdDgtDQ0OXLl6enp+/YsSM8PHzdunUPHjx4osyJEyfi4+MXLVpkPPwVCoWdgrGIWkFQ6WTQ/u1Wcy3O4knBZkpLS0+dOgUAYDKZQ4YM2bp1K5lMvnfv3hPFpFKpj8/fl8gLFy7YIxhrsHipMOdREM5SK+zysKWlpWXz5s07d+6sqqoqKSk5cOCAXq8XCoUAgODg4KKiouzs7JaWlp49e968efPOnTs4jn///ffGq01dXd3TFYaGhgIAMjIybOt+WkQtJwLCWGYKmPPoE0h/kCO3Q1QgISFh7dq1Z8+enThx4tSpU/Pz8/fs2WN0MXnyZIPBsGTJkuLi4qVLl/bt23f58uX9+/cXi8WbNm3q1avXkiVLnm6YQUFB48aN+/LLL3ft2mWPgB/myi08aTDTJ1LK8P0bSuzQG3M99q4rVitwMwXMnx8pQT3dxNUWbnU88zRU6kKj2Ey2ufOjhXEAkYncG+lN498UtFdg0aJFT18fAAA4jgMAqFTT9aenpxv7gNDJz89ftmyZyVU4jrcXDwDg4sWLJJLp6/GN9MakERaeLlh+PnNiV3XfUZ6BPUyfZRsbGzEMM7lKq9W218Uz/ka2EzU1NTZs1V5IlQ/Ut39vnrg40Pzmlj02VGjzr0tHzHi+Hs60knG4XjTY3TvIQp/f8i8W3xCGfzfGxaMN8GJzGS6kNQh6sCxKtPZ5YcwAPplMyjzdBCM2l+H6KTGNQbZyNEAHxgHkXZGoFfoXXrLqea6rcyO9ietOjbV6rE8H7kTEDXInU8HpA7W2xuYaGAwgfV8NnUm2XqIt46RKCpXnvq3tN8YrcbhHx4N0drJ/a8nOaB79mn9oBx+R2jhuL/N0U1GWLLofL6w32z+0Sx+E2YPaMk1pofJupjT2Rf4LL3nZUIPt40h1an3BdWnpXaWkURceyyVTAJtH4XvRcMwFXmyi0klSMaaUEXrCUFyg8PClh/VmCwe60xg2jkTs1HhcIxqlvrZUo5BiKhlhMACVHPKttvPnz48aNQpunW48CgmQ3HgUjjstIIzJdOvsHWsIHu1Nnz59bt265egoLIDeV4AD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuHgAh75fFsmeOpiXMCjVGrhXXxnwAU8ugTIIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEg/O+hxQfH08ikUikxxEaJ4+4ffu2o+MyjfO2R4FAQCaTSSQSmUw2fggIcN45o53XY3x8fNtjhSAI44RTzonzekxJSfH392/9GhgYOGvWLIdGZA7n9RgdHR0fH9/6VSQSRUdHOzQiczivRwDA9OnTjU3S399/5syZjg7HHE7tMSYmxnhOTEhIiIqKcnQ45oCTn8uIQQ9qStWSBkyjgjbb4cCY12QV3v2jxt7+vQVWnUw3iocvLSCMRYLXiqD1H2tLNdd+EZMAKaC7G252ynKHQ6WTa0qUAIB/TPSGNcs8HI8NldrLxxtHzAyk0lwm0xSuM2T8UD14io+vFdNFWQRCy9aq9Ce/rB49N8iFJBqn+hg9N+jEF1XmJ/y3EggeszNaEoa7ai6LhOHe2RkQzrwQPNaVq919aJ2vxyHwfeh1ZZrO1wPjuFbqWTyY1/2uhM2jqpUQehcQPBJ6g5kJyp0cgwHoCQjRO3U/3IVAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCwfEeX502Zt9+08l3xk0YcviHb8xvfuz4keHJfe0TWgdwjMdNm1edOfuzxWLTp82JjRF1RUCdxjEe7923KovWzJR5QmG8FQUdT1d71Ov1Q4cn1dfXbd+xZcKk4caFVCrt+PEjyaNeeHn84DXvL5fJZcblrcf1sWM/Tnl1VHl56Zx5rwwdnrTgjennz6c/XTlBEKkrl8x6bZJW29U5nLraI5lMPnfmOgBgZer6n0/8blx48dKvao1629YvUlesz8u7/e3BPU9sRaPT5XLZ5//Zuvq9TRcybg18ccj2T7eIxU+mSd+244NHxQ+2bf2iS1NEAgD5+bXNcDjcmSnzjJ+vXbtYkJ/zRAEymYxh2Ly5i6KiYgAAI0e+/N2hfY8e3ff2/ju74cHv9l68+OvnO/cJAizkLrIHjr9eAwDaXkx4fHetzvRRGRnZ2/iBy+UBABRKhXFcJIlEyvj93LcH96xdsyXqrzJdjFN4bJt+rL1kY+2tMhgMBEF8snWjsV3bLUYLOIXHzrPi3fdHjhz78ScbJBJow1c6xLPgkUwmjxk9fvmy1UwGc+v2zY6Joet3yWAwfHx879y5mZObbUxzCAUWi7V2zZasrOvHT6TBqtN6HNMeZ6bMz76dtX7DCp1OB7Ha3r2Fr81+fc/Xn7e0NEOs1hogjJM69K/yYTMEPE+XHFIhFWOXfqqZtaZbJ+t5Fs6PzgDyCAfkEQ7IIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4QPDI9aLhWld9YQHT6fleEO5UQfDI96A2Vqs7X49DEFdpeE7iMWaAe0mBvPP1OISSAnnMAAjzakPw6BNEFw7kX/lvXeer6mIuH60TDXb3CqB3vipo718X3pAVFyjZfKpvCAvKG1L2g0wmNVSoFRK8ZwI7uh8PSp0w50GSNGAV91XyFlwpg5naPjc3TySKg1ghm0flelK7RbrxvaE9C3He+aRaQXntnyOQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAcX8Ojt7QKTaruAR7FY7OgQLOMCHl0C5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wsF530MSiUQUCsU446hxMlK9Xp+T8+TUuU6C87ZHgUBgnPu2Na99UFCQo4NqF+f1KBKJ9Pq/M4YSBBEbG+vQiMzhvB6nT58uEAhavwYFBaWkpDg0InM4r0ehUNi2AQqFwpiYGEcGZBbn9QgASElJ8fX1Nea1nzFjhqPDMYdTe4yNjTWms4+Pj3fmxmhVXoCWBkxcrVXKYb6abj3D+yxQ1Hi/GDsp94rEIQFweFRvAcPd18Ib72b7jwaQfqBW3ozzfegMFgV+jK6ARknIm3U8L+pL8wLMFGvXo14Pjn9RHdXPPSSSbbcgXYbyIsX9bOnkpYHtZS1o1+PJr2oi+7gH9nCzb4CuQ9UD1cMcyfiFApNrTV9naks1JBIJSWxLUE83gx7Ul5tO3m7ao7hG68Z1itQ0TgWLQxXXmp6A37RHtZxg85HHJ2HzqSqp6X6LaY+wsr0/Y+j1oD0pTt0PdyGQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMckEc4II9wQB7h8Ix73LR51ZmzP3fBjp5xj/fu3+2aHZl+rpB1thnDQNxgT+sramoSb9226W5RfkhI2KQJU0vLim/eurF/7xEAgFjcuPvLz+4W5Wu12r59B8x5bWGgIAgA8OjRgzfeTNm96+DhHw5cv37Z19dv6JCRby5cZszPXFCQe/C7r+/fL/L08n6h38C5c95ksVgAgP8e++FI2nfL31m9afOqyZOmL1n8z8zMqxcuns/Lv6NQyKMiY2bPel0kSsRxPHnUC8bYeDy+Mff7mbM/n0o/XlZWHB4eMWzoqCmTp3dIVu6lZgYT9B1lQgu09rht++bKyvJPd3z1wabt165fun07y6gDx/F3UxcVFOamrlj/zf6fuFze4sWza+tqAAB0Oh0AsOPTLckjXvr1XObqVZvTfjp06XIGAKCiouy91UsxHNu96+DG9Z88fHjv3dRFxuE+NBpdrVYdSftu7Zot48e/olKpPvzX+ziOr1n9wUcf/jswMPj99f+USFqoVOq5M9cBACtT1xsl/vbbme07tkT2iv7x8Kl5cxf9dPTQ7i//DevPh+OxqUl881bm9OlzIntF+/j4rnj3/ZraKuOqvPw7lZXla1Z/0CfpBQ8Pz7cWv8vhcI8d+9GYbxkAMGRw8uBBw2k0Wrwoyc/P/8GDPwEAGb+fpVFpH2zaHhzcLTy8x4oV6+7du3sj8woAgEKhqFSqBfOXDBs6Migw2M3Nbd/eI8vfWR0vSooXJS18Y5lKpSoszHs6yFOnjwuF8e8sW+Xu7pGU2G/OawuPnzgik8ugGIDjsbSsuG16ej7fXSRKMn4uKMil0WgJ8X0e749MFsYlFBT8PYyxZ8+o1s8cDlehkAMACgvzIiN78/nuxuWBgiB/v4C8vDutJXv1jG79rFIq//N/216ZOnro8KRxE4YAACTSJ7OJ4zheVFTQJ6l/65L4+D4EQRj/bZ0HzkMYpVIBAGCyWK1LeFx+XV0NAEChkGMYNnR4UtvyXl5/v+JvbJVPoFDIHz66/8RWLS1NrZ+N5wQAQF1d7Tv/fL1PUv8N6z6Ojo4lCGL0Sy8+XaFGoyEIYv+B3fsP7G67XCqFM0wDjkcGnQEAINokBW+RPM5A7eXlzWKxPvrwf85EVIqF/Xp6eceyWPPmLmq7kM9zf7rkhYvnMQxb9d4mJpNpxguHw2EymaNHjRs0aHjb5SHBoVb8fZaB41EgCDIe3cHB3QAAMrksNzc7MDAYABAeHqFWq/39BQH+j5+gV9dUeXp4ma+we3jExYu/iuISSX8NYCgrKwkKCnm6pFQq4XJ5RokAAONlyiTh4RFqjTr+rxOOTqerr69te2R0Bjjnx5CQ0ODgbt8e3FNTWy1XyHfu/NhoFgDQr++Avn0HbN/+QX19nUTScvxE2qJFs87/mm6+wqlTZ+ME/sXuTzUaTUVF2Vd7Pp//+rTy8tKnS/bo3rOpSXz6zEkcx//Iul5YmMthcxoa6gAADAbDx8f3zp2bObnZOI6/+cayK1d+P3P2Z4Ig8vNzNm9ZvWLlYgzDoBiA1u9ZtXKjXq+fNXtiauri3tHCqMgYGvXxGK2PP9o5aNDwDz5cM2lK8s+/HB0zZsLECa+ar43P4+/fl8ZkMF9fOGPOvFfy8u+sWrmxe/eIp0uOGDFmZsq8b779KnnUCydOpr29dGXyyLGHvt//f7t2AABmpszPvp21fsMKnU4nFMbv+fL7/PycSZNHvLd6qVql+nDLZzQanNQp0PrhUqlEo9H4+fkbv763aimbzdm44RMoUToJXdEPX78x9d0Vb167dqmlpfngd3tzcrNffnkyrMqdH2jtUSJp2f7plvLy0qamxm4hYXNeW9i//z+ghup4zLRHaIN43N09PtryGazaXI5n/H5Pl4E8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuGAPMLBtEcm+zl9m9ACBsBqx4xpj57+9IYKV01Vbz/qK9Se/qaTjpv2GBzB0qj1KqhprF0dpRTHdPrA7iyTa9s5P5LAmDn+V0/U6zR60wWeM7Qq/bWT9S/N9Qcdfd8VACBpxH76d2X3OB7fm85we06vSFoFIW3WlRTIpy4PNpO/3fI8SEV/yBurtXBT1XeIoqKi6OhoKwraBTaP4hPEiO7HM1/MeeeTagXltX+OQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4YA8wgF5hAPyCAfkEQ7IIxyQRzggj3BAHuGAPMIBeYQD8ggH5BEOyCMcXMCjv7+/o0OwjAt4rKurc3QIlnEBjy4B8ggH5BEOyCMckEc4II9wQB7hgDzCAXmEA/IIB+QRDsgjHJBHOCCPcEAe4eC87yElJCQY09kbp4A0GAwGg+HOnTtWbOoAnLc9BgQEGNPZG7+SSKTAwEBHB9UuzutRKBS2PVb0er0D3zK0iPN6nDZtWtu89oGBgSivvS2IRKLIyMjWr0KhMC4uzqERmcN5PQIAZs6c6eXlBQDw8fGZNm2ao8Mxh1N7FIlExnT2MTExQqHQ0eGYA2YyXJWMUMlxpYzQqvQ6LQGlzuR+82VV/OF9phTekEKpkM4gM9wobB6FzaeyONCmhYHQf2yo0BYXKB/lKcg0qlaJUxkUOpuux5y0W0qmkXRKHa4jGG5UPY5HxHHCYth+IYxOVtspj/Xlmisnmgg9icJkcL3dmFzTc7I4LRq5Ti5W6bU6CkU/aKK3byds2u7xt8MNteVar1BPtgfT5t07CYpmTVNZsyCckTzD17YabPGokODff1IR1NuX4216MhsXRSFWVxc1zFrdjc3v8Hmzwx6lzfhPn1WG9wuiUJ36Wm8bBKYvzqqanhrM8+jYFbhjHsU12lP7GsL6CKwo68KU3qoev9Dfq50puEzSgTZlMIAjOyqfeYkAgLA+gT9uq+jQJh1oj8e+qOX4ezLYMLucTotWiSnrWya/FWBleWvbY+5liQ6jPCcSAQAMNk2jJeddtbbzb63HzNNNfhEdSLfwDOAX4Zl5usmKgsBajzmXJP4RnmRKO3PNPaNQqGT/7u55l61qklZ5LMyUsdydt7N99OePP901yx41M/iswj8geZQ141q1nslxsd98UGBx6So5oZBYnmvQssfyP5Xu/hxIgbkeHgJu2Z9Ki8UsX38bKrVkmh0bY9btX7KyT9bVFwf4R4hik//R//H92vUfjRiTvFgub/rt0n4mg90rov+El97lcb0AAFqt6vB/NzwqyQ7w6/Fiv1fsFxsAgESlNFbqQH8LxSy3R4WUoDLsNX3z7dyzR09+FCSIWrvi5KhhCy9fP/zL2c+Nq2g0xoUr39FojC1rM1YuSyspy/nt0n7jqp9OfiRuqlw8f/ecGVurax88ePSHncIDANAYVDmU41opxWl28/hH9snwbvGTx63ksD169uibPPT1a3+kKZXGXI4kX++QYYPmsFhcPs+nZ/e+1TX3AQBSWWNeYcbQgbODA6N5XK+XR71NpdjxcKEyKNbMxWrZI5VOIVPs4pEg8PLKgp4R/VqXRIQn6fVEafnjLLdBgX+nfmWxeGqNHADQ3FINAPDzDTMuJ5FIQYLIp+qGBplCptIs//mWz48UigHTYPb4JaPDNHo9cS7jq3MZX7VdLlc2//XRRI9VqZICAJiMvy99dLodb99hGpxqRYpDy3bYfKoG0sOWJ2AxOXQaMyn+ZWHvYW2Xe3sFmYvHjQ8AwHBt6xKN1vL11GZwLc7mW7ZkuYR3IKOi2F6ziAf4R+gwdY/wRONXDNe1tNS68/3MbOLhLgAAlFcWBAb0BADodJpHJdk8no+dItQTBm+B5fOv5fNjYHemrEEBKaonGTvyrfy7F7Ju/0IQRElZzqG0tXu+XYrhOjObuPN9Q0PizmV8JW6qxDDt4aPrSaYyP8NC1qBobw77tlhujwGhTK0SIzA9hQY/3PDQ+OWLDl64cjD93H9wQhcSFDNv5nYa1cL/f8aUjcdObf1s1yycwPomjE8Sjb3/MBN6bAAAXEdgGtyap4lW3X+8fLxJKqPx/NiQwnMZJLVKTw9s0CQLWaatvU8RP4TfUNxsRcFnjcaSpoShfGtKWtWb4XlSQ6PdmqvknkFckwVu3Dx25rfdJlcRBEahmO44pEzZHB050JoArOHSte8zLn9jchWLyVNrZCZXzZ/1aXg3kclVTZWy7rEcjrtViqx9rqBV6Y/trhX0Nj3FAYbrcExrcpUO09Bppu+50eksiqUE99aDYVq8nQsUjmPUdjqBZmKoKax75e0AOtOqQ7YDz2dK7yqvnZIEx7nAbBGdpyK3dvAkz26RblaW78AlOKw3u1eCW919sa2xuQy198TRfdjWS7RlHEBhpjw/UyWI8u54eK5BzZ/iuBfZvft17JZrh7uEMf25veLolXkuMIeJDVTm1UbGMzoq0fZxUhX31ZeOiTnebM9gq7oFzk9ThVTZpBj2qk9QhC13PWwfb6bHwfV0cVGWzDvUg+PFYrCtuCvifGgVmKJF3VjSEtOfP2Ccl82/MDs7jlSjJHIuSR/ckWOYge/HNQBAY1BoTBoATjqOFJAApsYxLQEAkNXJaQxSr0Ru/GD3TiYgg/Y+l1SM1ZRomut1Cilh0AOFBINSLXQ47jQSGXD4FE8/uiCcaSZ1WYdw3vfiXItncAyjQ0Ae4YA8wgF5hAPyCAfkEQ7IIxz+HxDUFTTxwYFRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b3d11",
   "metadata": {},
   "source": [
    "### 2개 모델을 연동한 코드를 Gradio를 사용해 UI로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c293c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n",
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\gradio\\utils.py:1028: UserWarning: Expected 2 arguments for function <function chatbot_interface at 0x0000020C7AF0E840>, received 1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\gradio\\utils.py:1032: UserWarning: Expected at least 2 arguments for function <function chatbot_interface at 0x0000020C7AF0E840>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-build-Gndk_Au9-py3.12\\Lib\\site-packages\\gradio\\helpers.py:1017: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 입력 질문: 파이썬이 뭐야?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 4776\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "Okay, so I'm trying to understand what Python is. From what I know, it's a programming language, but I'm not entirely sure how it works or why someone would use it. Let me start by breaking do...\n",
      "[DEBUG] generate 함수 - 질문: 파이썬이 뭐야?\n",
      "[DEBUG] generate 함수 - 추론 길이: 4776\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "Okay, so I'm trying to understand what Python is. From what I know, it's a programming language, but I'm not entirely sure how it works or why someone would use it. Let me start by breaking do...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 1170\n",
      "[DEBUG] 최종 응답 내용: 파이썬은 프로그래밍 언어 중 하나입니다. 이는 컴퓨터가 이해할 수 있는 명령어로 코드를 작성하여 실행하는 방식을 제공합니다. 파이는 \"프린트\"라는 함수를 사용하여 메시지를 출력하도록 키워드로 정의되어 있습니다.\n",
      "\n",
      "파이썬은 여러 가지 목적으로 사용될 수 있으며, 웹 개발과 데이터 분석, AI 및 머신러닝을 포함하여 다양한 분야에서 활용됩니다. 파이는 그 동안 뛰어난 학습성능에 대해 잘 알려져 있고, 이는 새로운 기술을 쉽게 배울 수 있는 기회를 제공합니다.\n",
      "\n",
      "파이썬의 구조는 디자인 시스템으로 이해할 수 있습니다. 예를 들어, 파이는 명령어 syntax를 사용하여 코드 작성하는 방식과 함께 정의된 정의적 (semantics)이 포함됩니다. 이를 통해 프로그램을 작동시키고 실행시킬 수 있도록 설계되었습니다.\n",
      "\n",
      "파이썬은 데이터 처리에 대한 유용한 라이브러리와 모듈이 존재하며, NumPy나 Pandas 같은 다양한 패키지가 제공되어 있습니다. 이들은 계산 및 분석을 위한 수학적 기능을 제공하여 파이썬에서 범위를 넓히는 것입니다.\n",
      "\n",
      "파이는 전통적으로 프로그래밍 언어 중 하나로 알려져 있지만, 다른 프로그래밍 언어들에 비해 더 유연하고 편리한 방식으로 데이터 처리 및 웹 개발을 위한 기능을 제공하는 것 같습니다. 파이썬은 Pythonic라는 용어를 사용하여 그 특성들을 잘 표현하며, 이는 프로그램을 쉽게 작성하고 이해하는데 도움이 될 수 있습니다.\n",
      "\n",
      "파이는 또한 AI와 Machine Learning에 익숙한 사람들에게 매우 유용할 수 있는 언어입니다. 파이썬에서는 Neural Networks를 구현하는 데 더 효과적인 모델이 만들어질 수 있을 것입니다.\n",
      "\n",
      "Python은 프로그래밍에서 다양한 기능을 제공하며, 그 기능들을 이해하고 활용하기 위해서는 자주 사용되는 예제와 테스트를 통해 더욱 확실하게 알아가야 합니다. 파이썬의 구조 및 특징들은 단순히 언어로만 이루어진 것이 아니라, 프로그래밍 전문성을 갖추기 위한 여러 가지 기능을 제공하므로, 이를 이해하고 활용하는 것이 중요합니다.\n",
      "\n",
      "파이는 프로그래밍 언어로서 다양한 분야에서 사용될 수 있는 강력한 도구를 제공하며, 파이썬에 대한 이해는 앞으로 나아가서 더 많은 문제를 해결할 수 있는 능력을 제공하게 됩니다. 파이썬의 구조와 특성은 다른 프로그래밍 언어들과 비교하여 어떻게 다르고, 무엇을 제공하는지 명확히 이해해야 합니다.\n",
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # 문자열이지만 인코딩 문제가 있을 수 있는 경우 처리\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # 문자열을 UTF-8로 인코딩했다가 다시 디코딩하여 정리\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "def launch_gradio():\n",
    "    iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "    iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #iface.launch()\n",
    "    launch_gradio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92803fa8",
   "metadata": {},
   "source": [
    "### LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85aa163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent 1] 원본 질문: LangGraph는 무엇이며, LangChain과 어떤 차이점이 있나요? 그리고 LangGraph를 사용해야 하는 이유는 무엇인가요?\n",
      "[Agent 1] 핵심 키워드: content='- LangGraph \\n- LangChain \\n- 차이점 \\n- 사용 이유' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 89, 'total_tokens': 109, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BmDKKxHXuBZQaXHVf7zsfEkfc35Wa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0ae1d2e1-4214-472e-bc4e-e7c0aba8f328-0' usage_metadata={'input_tokens': 89, 'output_tokens': 20, 'total_tokens': 109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "[Agent 1] 배경 정보: content='LangGraph는 다국어 간의 상호 연결을 쉽게 설계하고 관리할 수 있도록 도와주는 소프트웨어 툴이다. LangChain은 LangGraph를 통해 생성된 여러 언어 간의 연결된 데이터 구조로, 각 언어의 구조와 의미를 보존하면서 언어 간의 상호 작용을 가능하게 한다. \\n\\nLangGraph는 다국어 커뮤니케이션 및 통역, 번역 작업 등과 같은 다양한 다국어 환경에서의 작업을 보다 효율적으로 처리할 수 있도록 도와준다. LangGraph를 사용함으로써 시간과 비용을 절약하고, 번역 및 통역 작업의 정확성과 일관성을 향상시킬 수 있다. 또한, LangGraph를 통해 다양한 언어 간의 관계를 시각화하고 분석할 수 있어, 다국어 환경에서의 의사소통 및 협업을 원할하게 할 수 있다. \\n\\nLangGraph의 사용은 다국어 환경에서의 데이터 처리 및 관리를 보다 효율적으로 처리하기 위해 필요하며, LangChain과 같은 연결된 데이터 구조를 통해 다국어 간의 정보 교환 및 상호 작용을 간편하게 할 수 있다는 장점이 있다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 407, 'prompt_tokens': 91, 'total_tokens': 498, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BmDKLMSCUOMhs6VcQtFXIa8YhliE4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9e8b7e4a-9688-4476-86fe-6eeb355118d3-0' usage_metadata={'input_tokens': 91, 'output_tokens': 407, 'total_tokens': 498, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "[Agent 2] 최종 답변 생성 완료\n",
      "\n",
      "\n",
      "🔹 [AI 최종 답변]:\n",
      "LangGraph는 다국어 간의 상호 연결을 용이하게 설계하고 관리할 수 있는 소프트웨어 툴입니다. 반면에 LangChain은 LangGraph를 통해 생성된 여러 언어 간의 연결된 데이터 구조로, 각 언어의 구조와 의미를 보존하면서 언어 간의 상호 작용을 가능하게 합니다.\n",
      "\n",
      "LangGraph를 사용하는 이유는 다국어 커뮤니케이션, 통역, 번역 작업 등 다양한 다국어 환경에서 작업을 보다 효율적으로 처리할 수 있기 때문입니다. LangGraph를 사용함으로써 시간과 비용을 절약하고, 번역 및 통역 작업의 정확성과 일관성을 향상시킬 수 있습니다. 또한, LangGraph를 통해 다양한 언어 간의 관계를 시각화하고 분석할 수 있어 다국어 환경에서 의사소통 및 협업을 원할하게 할 수 있습니다.\n",
      "\n",
      "따라서, LangGraph를 사용함으로써 다국어 환경에서의 데이터 처리 및 관리를 보다 효율적으로 처리하고, LangChain과 같은 연결된 데이터 구조를 통해 다국어 간의 정보 교환 및 상호 작용을 간편하게 할 수 있는 장점이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 모델 설정\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# 첫 번째 AI 에이전트: 질문 분석 및 배경 정보 생성\n",
    "def agent_1(state):\n",
    "    \"\"\"사용자의 질문을 분석하고 핵심 키워드와 배경 정보를 추가\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # 질문에서 핵심 키워드 추출\n",
    "    keywords = llm.invoke(f\"질문: {query}\\n이 질문에서 핵심 키워드를 3~5개 추출해 주세요.\")\n",
    "    \n",
    "    # 질문과 관련된 배경 정보 제공\n",
    "    background_info = llm.invoke(f\"질문: {query}\\n이 질문을 이해하는 데 도움이 될 만한 추가 정보를 제공해 주세요.\")\n",
    "\n",
    "    print(f\"\\n[Agent 1] 원본 질문: {query}\")\n",
    "    print(f\"[Agent 1] 핵심 키워드: {keywords}\")\n",
    "    print(f\"[Agent 1] 배경 정보: {background_info}\\n\")\n",
    "\n",
    "    return {\"refined_query\": query, \"keywords\": keywords, \"background_info\": background_info}\n",
    "\n",
    "# 두 번째 AI 에이전트: 키워드 및 배경 정보를 활용하여 답변 생성\n",
    "def agent_2(state):\n",
    "    \"\"\"Agent 1이 제공한 정보를 기반으로 보다 정교한 답변 생성\"\"\"\n",
    "    refined_query = state[\"refined_query\"]\n",
    "    keywords = state[\"keywords\"]\n",
    "    background_info = state[\"background_info\"]\n",
    "\n",
    "    # Agent 1이 제공한 정보를 활용하여 최종 답변 생성\n",
    "    final_response = llm.invoke(\n",
    "        f\"질문: {refined_query}\\n\"\n",
    "        f\"핵심 키워드: {keywords}\\n\"\n",
    "        f\"배경 정보: {background_info}\\n\"\n",
    "        f\"위 정보를 바탕으로 질문에 대해 깊이 있는 답변을 작성해 주세요.\"\n",
    "    )\n",
    "\n",
    "    print(f\"[Agent 2] 최종 답변 생성 완료\\n\")\n",
    "    \n",
    "    return {\"final_answer\": final_response}\n",
    "\n",
    "# LangGraph Workflow 설정\n",
    "workflow = StateGraph(dict)  \n",
    "\n",
    "# 그래프의 시작점 정의\n",
    "workflow.add_node(\"agent_1\", agent_1)\n",
    "workflow.add_node(\"agent_2\", agent_2)\n",
    "\n",
    "# 실행 흐름(Edges) 정의\n",
    "workflow.set_entry_point(\"agent_1\")  # Agent 1이 먼저 실행됨\n",
    "workflow.add_edge(\"agent_1\", \"agent_2\")  # Agent 1 -> Agent 2\n",
    "\n",
    "# 실행 엔진 빌드\n",
    "app = workflow.compile()\n",
    "\n",
    "# 실행 예제\n",
    "query = \"LangGraph는 무엇이며, LangChain과 어떤 차이점이 있나요? 그리고 LangGraph를 사용해야 하는 이유는 무엇인가요?\"\n",
    "state = {\"query\": query}\n",
    "result = app.invoke(state)\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"\\n🔹 [AI 최종 답변]:\")\n",
    "print(result[\"final_answer\"].content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526a9a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cc65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-build-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
