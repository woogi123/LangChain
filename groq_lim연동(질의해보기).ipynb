{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b84546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f1b28",
   "metadata": {},
   "source": [
    "í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì§ˆë¬¸ input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a2090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"ìë°”ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fc2a0",
   "metadata": {},
   "source": [
    "ê²°ê³¼ ë°›ì•„ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9435812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000024077E4C890> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000240793E91F0> root_client=<openai.OpenAI object at 0x0000024076804620> root_async_client=<openai.AsyncOpenAI object at 0x00000240795C7FB0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82e3b3",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì„œ ë‹µì„ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0303c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ë‹µ: ìë°”ëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. 1995ë…„ì— ì¬ ë§ˆì´í¬ë¡œì‹œìŠ¤í…œì¦ˆ(í˜„ì¬ ì˜¤ë¼í´ ì†Œìœ )ì—ì„œ ì œì„ìŠ¤ ê³ ìŠ¬ë§ì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ìë°”ëŠ” í”Œë«í¼ ë…ë¦½ì ì´ë©°, ë‹¤ì–‘í•œ ìš´ì˜ ì²´ì œì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°**: ìë°”ëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ì´ëŠ” í”„ë¡œê·¸ë¨ì„ ê°ì²´ë¡œ êµ¬ì„±í•˜ê³ , ê°ì²´ ê°„ì˜ ìƒí˜¸ ì‘ìš©ì„ í†µí•´ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
      "2. **í”Œë«í¼ ë…ë¦½ì„±**: ìë°”ëŠ” ìë°” ê°€ìƒ ë¨¸ì‹ (JVM)ì„ í†µí•´ í”Œë«í¼ ë…ë¦½ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. JVMì€ ìë°” ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê°€ìƒ ë¨¸ì‹ ìœ¼ë¡œ, ë‹¤ì–‘í•œ ìš´ì˜ ì²´ì œì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "3. **ê°„ê²°í•œ ë¬¸ë²•**: ìë°”ëŠ” ê°„ê²°í•œ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìë°”ëŠ” C++ê³¼ ìœ ì‚¬í•œ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, ë”ìš± ê°„ë‹¨í•˜ê³  ëª…í™•í•©ë‹ˆë‹¤.\n",
      "4. **ê°•ë ¥í•œ ë³´ì•ˆ**: ìë°”ëŠ” ê°•ë ¥í•œ ë³´ì•ˆ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ìë°”ëŠ” ë©”ëª¨ë¦¬ ê´€ë¦¬, ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬, ë³´ì•ˆ ì •ì±… ë“±ì„ í†µí•´ ë³´ì•ˆ ìœ„í˜‘ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
      "5. **ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°**: ìë°”ëŠ” ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìë°”ëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, ë§ì€ ê°œë°œìë“¤ì´ ìë°”ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì›¹ ê°œë°œ, ëª¨ë°”ì¼ ì•± ê°œë°œ, ë°ìŠ¤í¬í†± ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ, ê²Œì„ ê°œë°œ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ìë°” ì–¸ì–´**: ìë°” ì–¸ì–´ëŠ” ìë°”ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ìë°” ì–¸ì–´ëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì„ ì§€ì›í•˜ë©°, ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "2. **ìë°” ê°€ìƒ ë¨¸ì‹ (JVM)**: JVMì€ ìë°” ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê°€ìƒ ë¨¸ì‹ ì…ë‹ˆë‹¤. JVMì€ í”Œë«í¼ ë…ë¦½ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "3. **ìë°” ë¼ì´ë¸ŒëŸ¬ë¦¬**: ìë°” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ìë°”ì˜ ê¸°ëŠ¥ì„ í™•ì¥í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ìë°” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ì˜ ë²„ì „ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ìë°” 1.0**: ìë°” 1.0ì€ ìë°”ì˜ ì²« ë²ˆì§¸ ë²„ì „ì…ë‹ˆë‹¤. 1995ë…„ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "2. **ìë°” 8**: ìë°” 8ì€ ìë°”ì˜ ìµœì‹  ë²„ì „ì…ë‹ˆë‹¤. 2014ë…„ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìë°”ëŠ” ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ìë°”ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ë©°, ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(\"ì‘ë‹µ:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173662bf",
   "metadata": {},
   "source": [
    "## LCEL\n",
    "Prompt + LLM Chainìœ¼ë¡œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206e159",
   "metadata": {},
   "source": [
    "1. PromptTemplate ì‚¬ìš©í•´ì„œ model í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c87d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an expert in AI Expert. Answer the question. <Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. \"\n",
    "                                \"<Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926a7b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain ì—°ê²°\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain ì—°ê²° (output_parser ì¶”ê°€)\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad6eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì‚¬ëŒì˜ ë‡Œê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. \n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**: ìš°ì„ , ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. \n",
      "\n",
      "2. **ë°ì´í„° ì „ì²˜ë¦¬**: ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ê¹¨ë—í•˜ê³  ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ëª¨ë¸ ì„¤ì •**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ì‹ ê²½ë§ì˜ ì¸µ ìˆ˜, ê° ì¸µì˜ ë‰´ëŸ° ìˆ˜, í™œì„±í™” í•¨ìˆ˜ ë“±ì˜ ê²°ì •ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
      "\n",
      "4. **í•™ìŠµ**: ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì…ë ¥í•˜ê³ , ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì‹¤ì œ ê²°ê³¼ ì‚¬ì´ì˜ ì˜¤ë¥˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë“¤(ê°€ì¤‘ì¹˜ì™€ í¸í–¥)ì„ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "5. **í‰ê°€**: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "6. **íŠœë‹**: ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šë‹¤ë©´, ëª¨ë¸ì˜ êµ¬ì¡°ë‚˜ í•™ìŠµ ë°©ë²•, ë°ì´í„°ì˜ ì „ì²˜ë¦¬ ê³¼ì • ë“±ì„ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ììœ¨ ì£¼í–‰ ìë™ì°¨ì˜ ê²½ìš°, ìˆ˜ë§ì€ ë„ë¡œ ìƒí™© ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ìë™ì°¨ê°€ ìŠ¤ìŠ¤ë¡œ ìƒí™©ì„ íŒë‹¨í•˜ê³  ìš´ì „í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
      "\n",
      "ì´ëŸ¬í•œ í•™ìŠµ ê³¼ì •ì„ í†µí•´ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥ì„ í‚¤ìš°ê²Œ ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Chain í˜¸ì¶œ\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ë°œìƒ : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8db8d",
   "metadata": {},
   "source": [
    "### Runnableì˜ stream()í•¨ìˆ˜ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "840000f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì‚¬ëŒì˜ ë‡Œê°€ í•™ìŠµí•˜ëŠ” ì›ë¦¬ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë‡ŒëŠ” ê²½í—˜ì„ í†µí•´ ë°°ìš°ê³ , ìƒˆë¡œìš´ ì •ë³´ë¥¼ ê¸°ì¡´ ì§€ì‹ê³¼ ì—°ê²°í•˜ì—¬ ê¸°ì–µí•©ë‹ˆë‹¤. ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë„ ë°ì´í„°ë¥¼ í†µí•´ ë°°ìš°ê³ , ê·¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ ì°¾ê³ , ë¯¸ë˜ì˜ ìƒˆë¡œìš´ ìƒí™©ì— ëŒ€ì²˜í•˜ëŠ” ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì§„ì„ ë³´ê³  ê³ ì–‘ì´ì¸ì§€ ê°œì¸ì§€ êµ¬ë¶„í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ë§Œë“ ë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. ì´ ëª¨ë¸ì—ê²Œ ìˆ˜ë§ì€ ê³ ì–‘ì´ì™€ ê°œì˜ ì‚¬ì§„ì„ ë³´ì—¬ì£¼ê³ , ì´ê²ƒì´ ê³ ì–‘ì´ì¸ì§€ ê°œì¸ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ëª¨ë¸ì´ ê³ ì–‘ì´ì™€ ê°œì˜ êµ¬ë¶„ì„ ì˜ ëª»í•˜ì§€ë§Œ, ì‚¬ì§„ì„ ë³¼ ë•Œë§ˆë‹¤ ì¡°ê¸ˆì”© í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ê´€ë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì´ ê²½ìš°, ê³ ì–‘ì´ì™€ ê°œì˜ ì‚¬ì§„ë“¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "2. **ë°ì´í„° ë¶„ì„**: ëª¨ë¸ì€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì€ ì‚¬ì§„ ì†ì—ì„œ ê³ ì–‘ì´ì™€ ê°œì˜ íŠ¹ì§•ì„ ì°¾ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ëŠ” ëˆˆì´ í¬ê³ , ê·€ê°€ ë¾°ì¡±í•˜ë©°, ê°œì˜ ê²½ìš° ê·€ê°€ ì³ì ¸ ìˆëŠ” ê²½ìš°ê°€ ë§ë‹¤ëŠ” ë“±ì˜ íŠ¹ì§•ì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ëª¨ë¸ ì—…ë°ì´íŠ¸**: ëª¨ë¸ì€ ë¶„ì„í•œ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ê³ ì–‘ì´ì™€ ê°œë¥¼ êµ¬ë¶„í•˜ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ, ë” ë§ì€ ë°ì´í„°ë¥¼ í•™ìŠµí• ìˆ˜ë¡ ì ì  ë” ì •í™•í•´ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì˜ˆì¸¡**: í•™ìŠµì´ ì™„ë£Œëœ í›„, ëª¨ë¸ì€ ìƒˆë¡œìš´ ì‚¬ì§„ì„ ë³´ê³  ê·¸ê²ƒì´ ê³ ì–‘ì´ì¸ì§€ ê°œì¸ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì´ëŸ¬í•œ í•™ìŠµ ê³¼ì •ì€ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë˜ë©°, ëª¨ë¸ì˜ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤. ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ëŸ¬í•œ ë°˜ë³µì ì¸ í•™ìŠµê³¼ ê°œì„  ê³¼ì •ì„ í†µí•´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•œ ìš”ì²­\n",
    "answer = chain2.stream({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬\"})\n",
    "# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "\n",
    "for token in answer:\n",
    "    # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë°›ì€ ë°ì´í„°ì˜ ë‚´ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ì„œ ì¶œë ¥í•˜ê³ , ë²„í¼ë¥¼ ì¦‰ì‹œ ë¹„ì›ë‹ˆë‹¤.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230c581",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "- prompt 1ì˜ ì¶œë ¥ì„ prompt 2ì˜ ì…ë ¥ì— ì‚¬ìš©í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¥ë¥´ì— ë”°ë¼ ì˜í™” ì¶”ì²œ\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} ì¥ë¥´ì—ì„œ ì¶”ì²œí•  ë§Œí•œ ì˜í™”ë¥¼ í•œ í¸ ì•Œë ¤ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# Step 2: ì¶”ì²œëœ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ ìš”ì•½\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} ì¶”ì „í•œ ì˜í™”ì˜ ì œëª©ì„ ë¨¼ì € ì•Œë ¤ì£¼ì‹œê³ , ì¤„ì„ ë°”ê¾¸ì–´ì„œ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ì²´ì¸ 1: ì˜í™” ì¶”ì²œ (ì…ë ¥: ì¥ë¥´ â†’ ì¶œë ¥: ì˜í™” ì œëª©)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64217427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  **ì¡´ ìœ… (John Wick)**\n",
      "2.  **ë¯¸ì…˜ ì„íŒŒì„œë¸”: ë°ë“œ ë ˆì½˜ë‹ íŒŒíŠ¸ ì›**\n",
      "\n",
      "*   **ì¡´ ìœ… (John Wick)**: ì€í‡´í•œ ì „ì§ ì•”ì‚´ì ì¡´ ìœ…ì€ ì€í‡´ í›„ í‰í™”ë¡œìš´ ì‚¶ì„ ì‚´ê³ ì í•˜ì§€ë§Œ, ê·¸ì˜ ì°¨ë¥¼ í›”ì¹˜ê³  ê°œë¥¼ ì£½ì¸ ê³ ì–´ê°±ì´ë¼ëŠ” ë²”ì£„ ì§‘ë‹¨ì´ ê·¸ì˜ ì‚¶ì— ìœ„í˜‘ì„ ê°€í•©ë‹ˆë‹¤. ì¡´ ìœ…ì€ ë³µìˆ˜ë¥¼ ìœ„í•´ ëª¨ë“  ê²ƒì„ í¬ìƒí•˜ë©° ì‹¸ì›ë‹ˆë‹¤. ê·¸ì˜ ì „ì„¤ì ì¸ ì‹¤ë ¥ê³¼ ì „íˆ¬ ê¸°ìˆ ì„ ë°”íƒ•ìœ¼ë¡œ ì¡´ ìœ…ì€ ë²”ì£„ ì¡°ì§ì— ë§ì„œ ì‹¸ìš°ë©°, ê°•ë ¥í•œ ì ë“¤ê³¼ì˜ ì¹˜ì—´í•œ ì „íˆ¬ë¥¼ ë²Œì…ë‹ˆë‹¤.\n",
      "*   **ë¯¸ì…˜ ì„íŒŒì„œë¸”: ë°ë“œ ë ˆì½˜ë‹ íŒŒíŠ¸ ì›**: ì „ ì„¸ê³„ì˜ í‰í™”ë¥¼ ìœ„í˜‘í•˜ëŠ” ê°•ë ¥í•œ ì¸ê³µì§€ëŠ¥ 'ì—”í‹°í‹°'ê°€ ë“±ì¥í•©ë‹ˆë‹¤. ë¹„ë°€ ì •ë³´ê¸°ê´€ IMFì˜ ì—ë‹¨ í—ŒíŠ¸(í†° í¬ë£¨ì¦ˆ)ëŠ” ë™ë£Œë“¤ê³¼ í•¨ê»˜ ì—”í‹°í‹°ë¥¼ íŒŒê´´í•˜ê¸° ìœ„í•œ ìœ„í—˜í•œ ì„ë¬´ì— ë‚˜ì„­ë‹ˆë‹¤. ì—ë‹¨ í—ŒíŠ¸ì™€ ê·¸ì˜ íŒ€ì€ ì—”í‹°í‹°ë¥¼ ë¬´ì°Œë¥´ê³  ì„¸ê³„ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ëª©ìˆ¨ì„ ê±´ ì „íˆ¬ë¥¼ ë²Œì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì²´ì¸ 2: ì¤„ê±°ë¦¬ ìš”ì•½ (ì…ë ¥: ì˜í™” ì œëª© â†’ ì¶œë ¥: ì¤„ê±°ë¦¬)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1ì˜ ì¶œë ¥ì„ movie ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰: \"SF\" ì¥ë¥´ì˜ ì˜í™” ì¶”ì²œ ë° ì¤„ê±°ë¦¬ ìš”ì•½\n",
    "response = chain2.invoke({\"genre\": \"ì•¡ì…˜\"})\n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463612fb",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d5313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deep learning is a subset of machine learning, which is a type of artificial intelligence (AI). It involves the use of artificial neural networks to analyze data and make decisions. The term \"deep\" refers to the fact that these neural networks are composed of multiple layers, which allow them to learn complex patterns and relationships in data.\n",
      "\n",
      "**Key Characteristics of Deep Learning:**\n",
      "\n",
      "* **Artificial Neural Networks (ANNs):** Deep learning models are based on ANNs, which are inspired by the structure and function of the human brain.\n",
      "* **Multiple Layers:** Deep learning models have multiple layers of interconnected nodes or \"neurons,\" which process and transform inputs to produce outputs.\n",
      "* **Hierarchical Representations:** Each layer in a deep learning model learns to represent the input data at a higher level of abstraction, allowing the model to learn complex patterns and relationships.\n",
      "\n",
      "**How Deep Learning Works:**\n",
      "\n",
      "1. **Data Input:** The model receives input data, such as images, text, or audio.\n",
      "2. **Forward Propagation:** The input data flows through the layers of the model, with each layer applying a set of weights and biases to the input data.\n",
      "3. **Activation Functions:** Each layer applies an activation function to the output, which determines the output of the layer.\n",
      "4. **Backward Propagation:** The model computes the error between its predictions and the actual output, and then propagates the error backwards through the layers to update the weights and biases.\n",
      "5. **Optimization:** The model is optimized using an optimization algorithm, such as stochastic gradient descent (SGD), to minimize the error and improve its performance.\n",
      "\n",
      "**Applications of Deep Learning:**\n",
      "\n",
      "* **Computer Vision:** Image classification, object detection, segmentation, and generation.\n",
      "* **Natural Language Processing (NLP):** Text classification, language translation, and text generation.\n",
      "* **Speech Recognition:** Speech-to-text and voice recognition.\n",
      "* **Robotics:** Control and navigation of robots.\n",
      "\n",
      "**Types of Deep Learning Models:**\n",
      "\n",
      "* **Convolutional Neural Networks (CNNs):** Used for image and video processing.\n",
      "* **Recurrent Neural Networks (RNNs):** Used for sequential data, such as text, speech, and time series data.\n",
      "* **Generative Adversarial Networks (GANs):** Used for generative modeling, such as generating new images or text.\n",
      "\n",
      "**Benefits of Deep Learning:**\n",
      "\n",
      "* **Improved Accuracy:** Deep learning models can achieve state-of-the-art performance on a wide range of tasks.\n",
      "* **Flexibility:** Deep learning models can be applied to a variety of domains and tasks.\n",
      "* **Scalability:** Deep learning models can be trained on large datasets and can scale to complex tasks.\n",
      "\n",
      "**Challenges of Deep Learning:**\n",
      "\n",
      "* **Training Time:** Deep learning models can take a long time to train, especially on large datasets.\n",
      "* **Overfitting:** Deep learning models can suffer from overfitting, which occurs when the model is too complex and performs poorly on new, unseen data.\n",
      "* **Interpretability:** Deep learning models can be difficult to interpret, making it challenging to understand why a particular decision was made.\n",
      "\n",
      "Overall, deep learning is a powerful tool for building AI systems that can learn from data and make accurate predictions or decisions. Its applications are diverse and continue to grow, transforming industries and revolutionizing the way we live and work.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an {topic} expert in AI. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ebc39",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "ì˜ˆì‹œë¥¼ ì¤˜ì„œ ê²°ê³¼ê°’ì„ ìœ ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc415c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### íƒœì–‘ê³„ì˜ í–‰ì„±\n",
      "1. **ìˆ˜ì„±**: ê°€ì¥ ì‘ì€ í–‰ì„±, íƒœì–‘ê³¼ ê°€ê¹ìŠµë‹ˆë‹¤.\n",
      "2. **ê¸ˆì„±**: ë§¤ìš° ëœ¨ê²ê³  ë°ì€ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "3. **ì§€êµ¬**: ìƒëª…ì²´ê°€ ì‚¬ëŠ” ìœ ì¼í•œ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "4. **í™”ì„±**: ë¶‰ì€ìƒ‰ì´ë©°, ë¡œë´‡ íƒì‚¬ê°€ í™œë°œí•©ë‹ˆë‹¤.\n",
      "5. **ëª©ì„±**: íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "6. **í† ì„±**: ì•„ë¦„ë‹¤ìš´ ê³ ë¦¬ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "7. **ì²œì™•ì„±**: ìì „ì¶•ì´ ê¸°ìš¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
      "8. **í•´ì™•ì„±**: ê°€ì¥ ë¨¼ í–‰ì„±ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\"})\n",
    "#result = chain.invoke({\"input\": \"ì–‘ì ì–½í˜ì´ ë¬´ì—‡ì¸ê°€ìš”?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea33ea",
   "metadata": {},
   "source": [
    "### PartialPromptTemplate\n",
    "* í”„ë¡¬í”„íŠ¸ ì…ë ¥ ê°’ì— í•¨ìˆ˜ í˜¸ì¶œ ì´ë‚˜ ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•œ ë™ì ì¸ ê°’ì„ ëŒ€ì…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60093c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ í”„ë¡¬í”„íŠ¸: ì—¬ë¦„ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ íƒœí’ ë°œìƒì…ë‹ˆë‹¤.\n",
      "ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: íƒœí’ì€ 7ì›”ì—ì„œ 9ì›” ì‚¬ì´ì— ì£¼ë¡œ ë°œìƒí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "    \n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì…ë‹ˆë‹¤.\",\n",
    "    input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "    partial_variables={\"season\": get_current_season()}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "query = prompt.format(phenomenon=\"íƒœí’ ë°œìƒ\")  # 'íƒœí’ ë°œìƒ'ì€ ì—¬ë¦„ê³¼ ê´€ë ¨ë¨\n",
    "result = model.invoke(query)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸ”¹ í”„ë¡¬í”„íŠ¸: {query}\")\n",
    "print(f\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={'info': '1ë‹¬ëŸ¬ = 1374.55ì›'} template='í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.'\n"
     ]
    }
   ],
   "source": [
    "# APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ë™ì ì¸ ê°’ì„  partial variableë¡œ ì„¤ì •\n",
    "\n",
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# {info} ë³€ìˆ˜ì— APIì—ì„œ ë°›ì€ í™˜ìœ¨ ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ë°˜ì˜\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c8ecadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ í”„ë¡¬í”„íŠ¸: í˜„ì¬ 1ë‹¬ëŸ¬ = 1374.55ì› ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\n",
      "ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: í˜„ì¬ í™˜ìœ¨ 1ë‹¬ëŸ¬ = 1374.55ì›ì€ í•œêµ­ ê²½ì œì— ì—¬ëŸ¬ ê°€ì§€ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ì„ , ë†’ì€ í™˜ìœ¨ì€ í•œêµ­ ìˆ˜ì¶œì—…ì²´ë“¤ì—ê²Œ ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ì¶œ ë¬¼í’ˆì˜ ê°€ê²©ì´ ì™¸í™”ë¡œëŠ” ë‚®ì•„ì ¸ì„œ í•´ì™¸ ì‹œì¥ì—ì„œ ê²½ìŸë ¥ì´ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì˜ ì£¼ìš” ìˆ˜ì¶œ ì‚°ì—…ì¸ ë°˜ë„ì²´, ìë™ì°¨, ì² ê°• ë“±ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŸ¬ë‚˜ ë†’ì€ í™˜ìœ¨ì€ ìˆ˜ì… ë¬¼í’ˆì˜ ê°€ê²©ì„ ìƒìŠ¹ì‹œì¼œ êµ­ë‚´ ì†Œë¹„ìë“¤ì—ê²Œ ë¶€ë‹´ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ìœ , ì „ìì œí’ˆ, ì˜ë¥˜ ë“± ìˆ˜ì…ì— ì˜ì¡´í•˜ëŠ” ì œí’ˆë“¤ì˜ ê°€ê²©ì´ ìƒìŠ¹í•˜ë©´, êµ­ë‚´ ë¬¼ê°€ ìƒìŠ¹ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ, ë¬¼ê°€ ìƒìŠ¹ì— ë¯¼ê°í•œ ì†Œë¹„ìë“¤ì—ê²Œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ë˜í•œ, ë†’ì€ í™˜ìœ¨ì€ ì™¸êµ­ì¸ íˆ¬ììë“¤ì—ê²Œ í•œêµ­ ìì‚°ì˜ ë§¤ë ¥ì„ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™¸êµ­ì¸ íˆ¬ììë“¤ì€ íˆ¬ì ìˆ˜ìµì„ ë‹¬ëŸ¬ë¡œ í™˜ì‚°í•˜ì—¬ ê°€ì ¸ê°€ê¸° ë•Œë¬¸ì—, ë†’ì€ í™˜ìœ¨ì€ ìˆ˜ìµì„ ê°ì†Œì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì˜ ì£¼ì‹ì‹œì¥ê³¼ ë¶€ë™ì‚° ì‹œì¥ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "í–¥í›„ í™˜ìœ¨ ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì€ ë‹¤ì–‘í•œ ë³€ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ë¯¸êµ­ì˜ í†µí™” ì •ì±…, í•œêµ­ì˜ ê²½ì œ ì„±ì¥ë¥ , ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©, ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ ë“± ì—¬ëŸ¬ ìš”ì¸ë“¤ì´ í™˜ìœ¨ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
      "\n",
      "í˜„ì¬ëŠ” ë¯¸êµ­ì˜ ê¸ˆë¦¬ ì¸ìƒê³¼ ê¸€ë¡œë²Œ ê²½ê¸° ì¹¨ì²´ ìš°ë ¤ ë“±ìœ¼ë¡œ ì¸í•´ ë‹¬ëŸ¬í™”ì˜ ê°•ì„¸ê°€ ì§€ì†ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í•œêµ­ ê²½ì œì˜ í€ë”ë©˜í„¸ì€ ì•ˆì •ì ì´ë©°, ìˆ˜ì¶œê³¼ ë‚´ìˆ˜ê°€ ê· í˜•ì„ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, í–¥í›„ í™˜ìœ¨ì€ ì ì°¨ ì•ˆì •ì„¸ë¥¼ ì°¾ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n",
      "\n",
      "ì¼ë¶€ ì „ë¬¸ê°€ë“¤ì€ ë¯¸êµ­ì˜ ê¸ˆë¦¬ ì¸ìƒ ì‚¬ì´í´ì´ ë§ˆë¬´ë¦¬ë˜ê³ , ê¸€ë¡œë²Œ ê²½ì œê°€ ì•ˆì •ì„¸ë¥¼ ì°¾ìœ¼ë©´, ë‹¬ëŸ¬í™”ì˜ ê°•ì„¸ê°€ ì ì°¨ ì•½í™”ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤. ì´ ê²½ìš°, í™˜ìœ¨ì€ 1ë‹¬ëŸ¬ = 1200ì›ëŒ€ê¹Œì§€ í•˜ë½í•  ìˆ˜ ìˆë‹¤ëŠ” ì „ë§ë„ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŸ¬ë‚˜, ì´ëŸ¬í•œ ì „ë§ì€ ë¶ˆí™•ì‹¤ì„±ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë³€ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, í™˜ìœ¨ì— ëŒ€í•œ íˆ¬ìëŠ” ì‹ ì¤‘í•˜ê²Œ ì´ë£¨ì–´ì ¸ì•¼ í•˜ë©°, ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê²°ë¡ ì ìœ¼ë¡œ, í˜„ì¬ í™˜ìœ¨ì€ í•œêµ­ ê²½ì œì— ê¸ì •ê³¼ ë¶€ì •ì ì¸ ì˜í–¥ì„ ëª¨ë‘ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. í–¥í›„ í™˜ìœ¨ ì˜ˆìƒê°’ì€ ë‹¤ì–‘í•œ ë³€ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§€ê² ì§€ë§Œ, ì „ë¬¸ê°€ë“¤ì€ ì ì°¨ ì•ˆì •ì„¸ë¥¼ ì°¾ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LLM ëª¨ë¸ ì„¤ì • (GPT-4o-mini ì‚¬ìš©)\n",
    "model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = model.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ”¹ í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ec8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-build-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
